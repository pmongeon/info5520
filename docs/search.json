[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research methods",
    "section": "",
    "text": "Overview\nThis book is a work-in-progress intended to support the course INFO5520 - Research Methods offered in the Master of Information program at Dalhousie University.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Research methods",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nIntroduction\nCourse overview, key concepts in research\n\n\n2\nConceptual phase\nLiterature review\n\n\n3\nConceptual phase\nResearch problems, purpose, and questions\n\n\n4\nMethodological phase\nMethodological designs, research ethics\n\n\n5\nMethodological phase\nSampling\n\n\n6\nMethodological phase\nDefining, operationalizing and measuring concepts\n\n\n7\nMethodological phase\nQualitative data collection\n\n\n8\nMethodological phase\nQuantitative data collection\n\n\n9\nAnalytical phase\nQualitative data analysis\n\n\n10\nAnalytical phase\nDescriptive and inferential statistics\n\n\n11\nReporting phase\nPrinciple of effective written, visual and oral presentation of research.\n\n\n12\nReporting phase\nGroup presentations\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUse the course Brightspace to:\n\nAccess the syllabus (under the content - overview tab).\nAccess the assignments instructions and due dates.\nSubmit your assignments.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#other-information-and-resources",
    "href": "index.html#other-information-and-resources",
    "title": "Research methods",
    "section": "Other information and resources",
    "text": "Other information and resources\n\nUse APA style for your references in all assignments.\nUse Zotero (or another reference manager of your choice) to store your references and to cite them in your work. (not mandatory, but highly recommended).\nYou can use this Word template to write your assignments (not mandatory). Whether you use this template or not, use Word styles to format and structure your document.\nHave fun.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  Research problem, purpose, and questions",
    "section": "",
    "text": "3.1 Slides",
    "crumbs": [
      "Conceptual phase",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Research problem, purpose, and questions</span>"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  What is research?",
    "section": "",
    "text": "1.1 Slides\nImportant note: The content for this week will be a set of slides that will be posted here. However, the slide deck might not be available before the class (Monday, January 6th). I will eventually complement the slides with more elaborated content and other resources.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is research?</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Literature review",
    "section": "",
    "text": "2.1 Slides",
    "crumbs": [
      "Conceptual phase",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  Methodological designs",
    "section": "",
    "text": "4.1 Slides",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methodological designs</span>"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Research ethics",
    "section": "",
    "text": "5.1 Guiding principles\nAt the end of this chapter, you should be familiar with the Tri-Council Policy Statement on the Ethical Conduct for Research Involving Humans – TCPS 2 (2022). More specifically, you should be familiar with:\nThe three guiding principles of the TPSC 2 are:",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6  Sampling",
    "section": "",
    "text": "6.1 Objectives\nAt the end of this chapter, you should be able to :",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "",
    "text": "7.1 Learning objectives",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch8.html",
    "href": "ch8.html",
    "title": "8  Qualitative data collection",
    "section": "",
    "text": "Under construction….",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative data collection</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "9  Quantitative data collection",
    "section": "",
    "text": "9.1 Slides",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative data collection</span>"
    ]
  },
  {
    "objectID": "ch10.html",
    "href": "ch10.html",
    "title": "10  Qualitative data analysis",
    "section": "",
    "text": "10.1 Learning objectives\nAt the end of the chapter, the reader should:",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html",
    "href": "ch11.html",
    "title": "11  Quantitative data analysis",
    "section": "",
    "text": "11.1 Processing data\nBy the end of this chapter, you will be able to:\nData is stored in all kinds of places, can be accessed in many different ways, and comes in all shapes and forms. Therefore, much of the researcher’s work can sometimes be related to the processing and cleaning of the data to get it ready for analysis. Here are a few principles to structure data in a tabular format (e.g., an Excel spreadsheet) in a way that maximizes the data usability:\nIn some circles, data that follows these principles is called tidy data.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch12.html",
    "href": "ch12.html",
    "title": "12  Principles of effective communication of research",
    "section": "",
    "text": "Under construction…",
    "crumbs": [
      "Reporting phase",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principles of effective communication of research</span>"
    ]
  },
  {
    "objectID": "ch13.html",
    "href": "ch13.html",
    "title": "13  Descriptive and inferential statistics",
    "section": "",
    "text": "Under construction…",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Descriptive and inferential statistics</span>"
    ]
  },
  {
    "objectID": "ch14.html",
    "href": "ch14.html",
    "title": "14  Principles of effective communication of research",
    "section": "",
    "text": "Under construction…",
    "crumbs": [
      "Reporting phase",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Principles of effective communication of research</span>"
    ]
  },
  {
    "objectID": "ch1.html#slides",
    "href": "ch1.html#slides",
    "title": "1  What is research?",
    "section": "",
    "text": "Click here to open the slides in a new tab",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is research?</span>"
    ]
  },
  {
    "objectID": "ch2.html#slides",
    "href": "ch2.html#slides",
    "title": "2  Literature review",
    "section": "",
    "text": "2.1.1 January 27 guest lecture\nDownload the slides here.\n\n\n2.1.2 Regular class\n\nClick here to open the slides in a new tab",
    "crumbs": [
      "Conceptual phase",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "ch3.html#slides",
    "href": "ch3.html#slides",
    "title": "3  Research problem, purpose, and questions",
    "section": "",
    "text": "Click here to open the slides in a new tab",
    "crumbs": [
      "Conceptual phase",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Research problem, purpose, and questions</span>"
    ]
  },
  {
    "objectID": "ch4.html#slides",
    "href": "ch4.html#slides",
    "title": "4  Methodological designs",
    "section": "",
    "text": "Click here to open the slides in a new tab",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methodological designs</span>"
    ]
  },
  {
    "objectID": "ch4.html#example-of-articles-using-methodological-designs",
    "href": "ch4.html#example-of-articles-using-methodological-designs",
    "title": "4  Methodological designs",
    "section": "4.2 Example of articles using methodological designs",
    "text": "4.2 Example of articles using methodological designs\n\n4.2.1 Phenomenology\nGarner, J. (2019). ‘A Little Happy Place’: How Libraries Support Prisoner Wellbeing. Journal of the Australian Library and Information Association, 68(4), 343–355. https://doi.org/10.1080/24750158.2019.1670774\n\n\nview abstract\n\nThis article explores the capacity for Australian prison libraries to support the ‘whole person’. Based on a phenomenological study of the experiences of using prison libraries by Australian adult prisoners, the article identifies three ways that prison libraries can reduce stress, and support positive mental health, and thereby serve the ‘whole person’ within their user groups. The study finds that prison libraries can support prisoner wellbeing by providing opportunities for autonomy, by acting as therapeutic spaces, and by supporting positive behaviour management. As a phenomenological study, the experiences of the prisoners regarding their libraries serving them as ‘whole people’ are described in their own words. We can hear the prisoners’ voices as they describe how their libraries contribute to their wellbeing. Through a study of prisoners’ lived experiences and how these experiences are reflected, or not reflected in the extant literature, it is possible to demonstrate that prison libraries do have the capacity to serve the ‘whole person’ and thereby support prisoner wellbeing.\n\nPeuler, M. (2024). Mother librarians on the tenure track: A phenomenological study of their experiences. The Journal of Academic Librarianship, 50(3), 102866. https://doi.org/10.1016/j.acalib.2024.102866\n\n\nview abstract\n\nThis exploratory qualitative phenomenological research study examines the lived experiences of thirteen academic librarians who identify as mothers of young children and are also on the tenure track or continuous appointment at an academic library in the United States. The qualitative approach of phenomenology is used to both analyze and interpret academic librarian mothers’ perceptions of their experiences. Phenomenology allows the researcher to explore the phenomena in relation to the lived experiences of mother/tenure track librarians while simultaneously seeking to understand these phenomena. This study provides valuable insight into the perceptions of mothers regarding their experiences in balancing their professional and parenting responsibilities. Results from the thematic analysis revealed six major Themes: 1) Specific tenure/continuation process impact factors; 2) Remote and flexible work options; 3) Scholarship challenges; 4) Time; 5) Librarian-manager relationship; and 6) Parental leave. Giving individuals the opportunity to share their experiences benefits not only the individuals but also the profession overall as we gain insight into the world of others through conclusions drawn from the analysis of their responses.\n\nSean Burns, C., & Bossaller, J. (2012). Communication overload: A phenomenological inquiry into academic reference librarianship. Journal of Documentation, 68(5), 597–617. https://doi.org/10.1108/00220411211255996\n\n\nview abstract\n\nPurpose This study aims to provide insight on the meaning of communication overload as experienced by modern academic librarians. Communication is the essence of reference librarianship, and a practically endless array of synchronous and asynchronous communication tools (ICTs) are available to facilitate communication.\nDesign/methodology/approach This study relied on a phenomenological methodology, which included nine in‐depth interviews with academic librarians. The interviews were transcribed and analyzed using RQDA, a qualitative analysis software package that facilitates coding, category building, and project management.\nFindings Seven themes about librarianship emerged from this research: attending to communication abundance, librarians of two types, instruction not reference, twenty‐first century librarianship, user needs, trusted methods: filter not retrieve, and self‐impact. The shared meaning of communication overload among these librarians is that it is a problem when it detracts from or hinders their ability to assist their users.\nPractical implications Further research should contribute to an understanding of communication as a problem when it interferes with serving the librarians’ users, or to an understanding of interpersonal communication within the librarians’ organizational structures and in their broader professional networks.\nSocial implications Research in popular psychology has focused on the negative impacts on productivity and concentration of living in an always‐plugged‐in environment. This research confirms that librarians should have time to work away from digital distractions to maintain job satisfaction.\nOriginality/value Important work by Radford and Dervin has focused on communication with users. This study focuses on the impact of ICTs on librarians’ work and personal lives.\n\n\n\n4.2.2 Ethnography\nAhoketo, P., & Supranta, J. (2024). “This Building is Ours!” Student Activism Against the University’s Neoliberal Policy. Journal for Critical Education Policy Studies, 22(1), 1–43.\n\n\nview abstract\n\nThis article is an ethnography of a student protest against a Finnish university’s plans to give up 25 percent of its campus buildings until 2030. The Finnish universities faced financial deficits primarily due to education cuts implemented by Finland’s right-wing government between 2015 and 2019. To balance the budget, Tampere University proposed surrendering some of its buildings, including the Linna, the home of social sciences, and the main library. The students organized the We Will Not Give Up the Linna Building movement (WWGU) to oppose the university’s decision. This article is an ethnography of the movement’s resistance and outcomes and analyzes what the student activists learned and how they changed during the protest wave in 2021. Our analysis uncovered six key insights the student activists learned on democracy, social media in activism, activism’s temporality and persistence, the role of emotions in activism, and the university’s power structures. The study contributes to a general understanding of the student protest movement, the social transformations that student activists undergo, and how they learn to perceive democracy, develop political imagination, and understand power structures. © 2024, Institute for Education Policy Studies. All rights reserved.\n\nCavanagh, M. F. (2013). Interpreting reference work with contemporary practice theory. Journal of Documentation, 69(2), 214–242. https://doi.org/10.1108/00220411311300057\n\n\nview abstract\n\nPurpose: In an increasingly competitive field of socially mediated information and knowledge available online, the public library’s traditional services are increasingly questioned for relevancy. Drawing on the core premises of contemporary practice theory to ground the methodological and theoretical perspectives, the aim of this paper is to provide the initial “inside” view of traditional public library face-to-face reference work from a practice-based perspective. Design/methodology/approach: The paper includes an ethnographic case study of face-to-face reference service in four branches of one urban public library involving 170 hours of participant observation, 24 hours of unobtrusive observation, 480 reference interactions, and 28 participant interviews and analysis of policy documents. Findings: This analysis highlights the structuring and mediating role of objects in the enactment of reference work. A practice-based typology of reference interactions is introduced which characterizes the types of questions asked, knowledge processes in action, interpersonal communication style and mode of practice. The collective organizing actions of reference work are unpackaged in a non-hierarchical or flattened plane that recognizes the key actors and dynamics of the practice as it endures across time and space. Originality/value: Evidence and an approach are introduced to support re-conceptualizing public library reference work as an epistemic practice. © Emerald Group Publishing Limited.\n\nTomlin, N., Tewell, E., Mullins, K., & Dent, V. (2017). In Their Own Voices: An Ethnographic Perspective on Student Use of Library Information Sources. Journal of Library Administration, 57(6), 631–650https://doi.org/10.1080/01930826.2017.1340776\n\n\nview abstract\n\nA medium-sized academic library system conducted a multi-year (2012–2016), large-scale ethnographic study using a survey, observations, and in-depth interviews. The goal of the project was to better understand how students conduct research and study in the library. The analysis of the large pool of data resulted in various reports of theoretical and practical nature. This article addresses one aspect of the findings: undergraduate and graduate student use of library resources. The findings offer an array of considerations for designing effective library services and provide a more nuanced understanding of how student use the library website, libguides, and library databases as well as print and electronic collections. © 2017, Published with license by Taylor & Francis © 2017, © Natalia Tomlin, Eamon Tewell, Kimberly Mullins, and Valeda Dent.\n\n\n\n4.2.3 Case study\nBryant, J., Matthews, G., & Walton, G. (2009). Academic libraries and social and learning space: A case study of Loughborough university library, UK. Journal of Librarianship and Information Science, 41(1), 7–18https://doi.org/10.1177/0961000608099895\n\n\nview abstract\n\nA key area of debate within the public and academic library sectors across the world is use of physical space. Changing ideas about what a library should be, coupled with the growth of digital collections, has raised fundamental questions about how library buildings are used and the role of space in library services. Alongside these drivers is the need for libraries to produce data on services to inform their future development and design. This article is a case study from Loughborough University in the UK to evaluate the use of a large open learning/social space in the library. The investigation employs an ethnographic approach to gather data, a method little used in the field. Findings are explored under the following themes: collaborative study, individual study, social space, intrusions and interruptions, use of technology, diversity, library staff/library materials and spatial organization. The role of ethnographic studies within the library context is considered alongside the broader theoretical considerations of the use of physical space.\n\nDabner, N. (2012). “Breaking Ground” in the use of social media: A case study of a university earthquake response to inform educational design with Facebook. Internet and Higher Education, 15(1), 69–78. https://doi.org/10.1016/j.iheduc.2011.06.001\n\n\nview abstract\n\nOn September 4 2010, a massive 7.1 magnitude earthquake struck the Canterbury region in the South Island of New Zealand. The response from the University of Canterbury was immediate and carefully co-ordinated, with the university’s web-based environment and a responsive site developed on the social media platform ‘Facebook’ becoming prominent sources of support for many months. This case study illustrates how the university effectively utilised these environments and their impact within the wider university community. Case study methodology draws upon literature from the fields of social media, social network communities and crisis informatics. The findings propose that social media can effectively support information sharing, communication and collaboration in higher education contexts, in particular in times of crisis, but suggest there needs to be a defined purpose to integrate these within an institution’s communications strategy given the resource implications and range of social media already used by students. © 2011 Elsevier Inc.\n\nVakkari, P., Pennanen, M., & Serola, S. (2003). Changes of search terms and tactics while writing a research proposal: A longitudinal case study. Information Processing and Management, 39(3), 445–463. Scopus. https://doi.org/10.1016/S0306-4573(02)00031-6\n\n\nview abstract\n\nThe study analyses how students’ growing understanding of the topic and search experience were related to their choice of search tactics and terms while preparing a research proposal for a small empirical study. In addition to that, the findings of the study are used to test Vakkari’s (2001) theory of task-based IR. The research subjects were 22 students of psychology attending a seminar for preparing the proposal. They made a search for their task in PsychINFO database at the beginning and end of the seminar. Data were collected in several ways. A pre- and post-search interview was conducted in both sessions. The students were asked to think aloud in the sessions. This was recorded as were the transaction logs. The results show that search experience was slightly related to the change of facets. Although the students’ vocabulary of the topic grew generating an increased use of specific terms between the sessions, their use of search tactics and operators remained fairly constant. There was no correlation between the terms and tactics used and the total number of useful references found. By comparing these results with the findings of relevant earlier studies the conclusion was drawn that domain knowledge has an impact on searching assuming that users have a sufficient command of the system used. This implies that the tested theory of task-based IR is valid on condition that the searchers are experienced. It is suggested that the theory should be enriched by including search experience in its scope. © 2002 Elsevier Science Ltd. All rights reserved.\n\n\n\n4.2.4 Descriptive\n\n4.2.4.1 Simple\nMongeon, P., Gracey, C., Riddle, P., Hare, M., Simard, M.-A., & Sauvé, J.-S. (2023). Mapping information research in Canada. The Canadian Journal of Information and Library Science / La Revue canadienne des sciences de l’information et de bibliothéconomie, 46(2), 1–27. https://doi.org/10.5206/cjils-rcsib.v46i2.15568\n\n\nview abstract\n\nThis study examines the Canadian information research landscape through the lens of the eight academic units hosting ALA-accredited programs. We created a citation-based network utilizing the scholarly articles published by the faculty members and PhD students at each academic unit to identify and characterize distinct research clusters within the field. Then we determined how the publications and researchers from each unit are distributed across the clusters to describe their area of specialization. Our findings emphasize how the inter-, multi-, and transdisciplinary nature of the Canadian information research landscape forms a rich mosaic of information scholarship.\n\n\n\n4.2.4.2 Comparative\nRiddle, P., Simard, M.-A., Gone, P., Li, V., & Mongeon, P. (2023). The state of green open access in Canadian universities. The Canadian Journal of Information and Library Science / La Revue canadienne des sciences de l’information et de bibliothéconomie, 46(2), 1–25. https://doi.org/10.5206/cjils-rcsib.v46i2.15358\n\n\nview abstract\n\nThis study investigates the use of institutional repositories for self-archiving peer-reviewed work in the U15 (an association of fifteen Canadian research-intensive universities). It relates usage with university open access (OA) policy types and publisher policy embargoes. We show that of all articles found in OpenAlex attributed to U15 researchers, 45.1 to 56.6% are available as Gold or Green OA, yet only 0.5 to 10.7% (mean 4.2%) of these can be found on their respective U15 IRs. Our investigation shows a lack of OA policies from most institutions, journal policies with embargoes exceeding 12 months, and incomplete policy information.\n\nSimard, M.-A., Ghiasi, G., Mongeon, P., & Larivière, V. (2022). National differences in dissemination and use of open access literature. PLOS ONE, 17(8), e0272730. https://doi.org/10.1371/journal.pone.0272730\n\n\nview abstract\n\nOpen Access (OA) dissemination has been gaining a lot of momentum over the last decade, thanks to the implementation of several OA policies by funders and institutions, as well as the development of several new platforms that facilitate the publication of OA content at low or no cost. Studies have shown that nearly half of the contemporary scientific literature could be available online for free. However, few studies have compared the use of OA literature across countries. This study aims to provide a global picture of OA adoption by countries, using two indicators: publications in OA and references made to articles in OA. We find that, on average, low-income countries are publishing and citing OA at the highest rate, while upper middle-income countries and higher-income countries publish and cite OA articles at below world-average rates. These results highlight national differences in OA uptake and suggest that more OA initiatives at the institutional, national, and international levels are needed to support wider adoption of open scholarship.\n\n\n\n\n4.2.5 Correlational\n\n4.2.5.1 descriptive\nHare, M., Krause, G., MacKnight, K., Bowman, T. D., Costas, R., & Mongeon, P. (2024). Do you cite what you tweet? Investigating the relationship between tweeting and citing research articles. Quantitative Science Studies, 5(2), 332–350. https://doi.org/10.1162/qss_a_00296\n\n\nview abstract\n\nThe last decade of altmetrics research has demonstrated that altmetrics have a low to moderate correlation with citations, depending on the platform and the discipline, among other factors. Most past studies used academic works as their unit of analysis to determine whether the attention they received on Twitter was a good predictor of academic engagement. Our work revisits the relationship between tweets and citations where the tweet itself is the unit of analysis, and the question is to determine if, at the individual level, the act of tweeting an academic work can shed light on the likelihood of the act of citing that same work. We model this relationship by considering the research activity of the tweeter and its relationship to the tweeted work. The results show that tweeters are more likely to cite works affiliated with their same institution, works published in journals in which they also have published, and works in which they hold authorship. It finds that the older the academic age of a tweeter the less likely they are to cite what they tweet, though there is a positive relationship between citations and the number of works they have published and references they have accumulated over time.\n\n\n\n4.2.5.2 predictive\n\n\n\n4.2.6 Experimental\n\n\n4.2.7 Quasi-experimental\n\n\n4.2.8 Mixed-methods\nBowles-Terry, M. (2012). Library instruction and academic success: A mixed-methods assessment of a library instruction Program. Evidence Based Library and Information Practice, 7(1), 82–95. https://doi.org/10.18438/b8ps4d\n\n\nview abstract\n\nObjectives - This study examines the connection between student academic success and information literacy instruction. Locally, it allowed librarians to ascertain the institution’s saturation rate for information literacy instruction and identify academic programs not utilizing library instruction services. In a broader application, it provides an argument for a tiered program of information literacy instruction and offers student perspectives on improving a library instruction program. Methods - Focus groups with 15 graduating seniors, all of whom had attended at least one library instruction session, discussed student experiences and preferences regarding library instruction. An analysis of 4,489 academic transcripts of graduating seniors identified differences in grade point average (GPA) between students with different levels of library instruction. Results - Students value library instruction for orientation purposes as beginning students, and specialized, discipline-specific library instruction in upper-level courses. There is a statistically significant difference in GPA between graduating seniors who had library instruction in upper-level courses (defined in this study as post-freshman-level) and those who did not. Conclusions - Library instruction seems to make the most difference to student success when it is repeated at different levels in the university curriculum, especially when it is offered in upper-level courses. Instruction librarians should differentiate between lower-division and upper-division learning objectives for students in order to create a more cohesive and non-repetitive information literacy curriculum. © 2012 Bowles-Terry.\n\nChan, T. T. W., Lam, A. H. C., & Chiu, D. K. W. (2020). From Facebook to Instagram: Exploring user engagement in an academic library. Journal of Academic Librarianship, 46(6). https://doi.org/10.1016/j.acalib.2020.102229\n\n\nview abstract\n\nIt is trendy for university libraries to offer services on social media (SM) platforms. As many millennials prefer to use apps of photo-snapping, libraries unavoidably start to use Instagram to connect with users. This study examines the effectiveness of the use of Facebook and Instagram in The University of Hong Kong Libraries (HKUL), by a mixed-method approach to analyze the posts on the two SM sites of HKUL and user feedback collected from interviews on campus. The content analysis surprisingly revealed that both Facebook and Instagram received very low user engagement, and the most frequently posted contents were library operational news and library events. The interviewees opined that they welcomed HKUL’s use of Facebook and Instagram, but they urged HKUL to change its uninteresting style on the SM sites and make better use of the SM features to attract users’ attention. This study provides insights for librarians for their effective management and adoption of SM, especially for content design and management. © 2020 Elsevier Inc.\n\nColón-Aguirre, M., & Bright, K. M. (2024). “So, That Would Have Been Useful”: Curriculum in LIS in Support of Liaison Librarian Preparation. Journal of Education for Library and Information Science, 65(4), 373–389. https://doi.org/10.3138/jelis-2023-0022\n\n\nview abstract\n\nLIS education has historically come under fire for what some perceive as a disconnect between what is taught in the classroom and what the job really entails. This study is part of a larger research study that used a sequential explanatory mixed-methods design to investigate liaison librarians’ perceptions of their academic preparation to take on the liaison role, specifically whether and how their LIS program curriculum prepared them for this role. This qualitative strand of the study relied on in-depth semi-structured interviews of survey participants to explore two research questions: What are the perceptions of academic liaison librarians regarding the degree to which their programs prepared them for their current role? And which factors influenced these perceptions? This research identified the main reasons hindering the effectiveness of LIS education for preparing librarians for the liaison role to be a range of complex issues that LIS programs and educators should consider, such as changing career plans among students who did not initially plan to become liaison librarians, students not taking certain courses due to scheduling or course timing issues, and the abstract nature of some course content which obscures connections to real-world practice. Participants also identified courses in collection development, reference, instruction, and research methods as those that should take center stage when preparing liaison librarians. These results have the potential to inform various aspects of LIS program curriculum planning and design and provide course-selection guidance for LIS students considering a career in academic libraries. © Association for Library and Information Science Education, 2024.\n\nWedlake, S., Coward, C., & Lee, J. H. (2024). How games can support misinformation education: A sociocultural perspective. Journal of the Association for Information Science and Technology, 75(13), 1480–1497. https://doi.org/10.1002/asi.24954\n\n\nview abstract\n\nThis study uses a sociocultural perspective, which views literacy as embedded in people’s daily practices and shaped by social contexts, to explore how a misinformation escape room can support learning about misinformation. While the sociocultural perspective has a rich theoretical foundation, it has rarely been used to examine, much less evaluate, information and media literacy interventions. In this paper, we posit that the topic of misinformation makes a strong case for using the sociocultural model and explore a misinformation escape room through this lens. We present findings of a nationwide study of an online misinformation escape room with post-game debrief discussion conducted at 10 public libraries that hosted 53 game sessions involving 211 players. The mixed methods study finds the game and accompanying debrief supported players in reflecting upon social media platform infrastructures, the psychological and emotional dimensions of misinformation, and how their personal behaviors intersect with online misinformation. We discuss how the sociocultural perspective can enrich our understanding of the role played by certain attributes of the game—narrative, debrief, and collaboration—thereby providing insights for the design of media and information literacy interventions. © 2024 Association for Information Science and Technology.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methodological designs</span>"
    ]
  },
  {
    "objectID": "ch6.html#key-concepts",
    "href": "ch6.html#key-concepts",
    "title": "6  Sampling",
    "section": "6.2 Key concepts",
    "text": "6.2 Key concepts\n\n6.2.1 Population\nIn research, the population refers to the group (e.g., people, objects, events) about which we want to know something. For examples, if we want to know how online delivery of courses affect the undergraduate student experience, then our population would be all undergraduate students.\nThe Target population is the population to which you can hope to generalize, it’s the subset of the population that meets the selection crietria. The selection criteria are aimed at reducing heterogeneity in the population, and eventually reduce the sample size requirement, and improve the likelihood that your study wil generate credible knowledge. For example, if your study aims to understand the experience of first first-year undergraduate students at a university, you may choose to set specific criteria that would create undesired heterogeneity in your population, such as requiring that they are between 18 and 21 years old and enrolled full-time in a program in which in-person courses are the norm. This will prevent you from dealing with the potentially different experiences of students who study online, of those who are mainly working and taking courses part-time, or who are older adults returning to university or doing a second degree.\nThe accessible populationis the subset of the target population that is accessible to the researcher, and from which a sample will be drawn. It should be as representative as possible of the target population. For example, if your accessible population are residents of Halifax, then it might be a bad idea to aim to generalize your results to people living in rural regions of south america (that would be silly), but your target population could be medium-sized cities in the Atlantic provinces of Canada, for example.\n\n\n6.2.2 Sample\nThe sample is the subset of the accessible population that is included in a study. It is drawn from the accessible population, following a probability or non-probability sample method, which are described in the next section. Sampling tends to be a very important step of a study as it will determine the degree to which your findings can produce convincing and reliable knowledge about your target population.\n\n\n\nSampling from a population",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#sampling-methods",
    "href": "ch6.html#sampling-methods",
    "title": "6  Sampling",
    "section": "6.3 Sampling methods",
    "text": "6.3 Sampling methods",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#choice",
    "href": "ch6.html#choice",
    "title": "6  Sampling",
    "section": "6.6 Choice",
    "text": "6.6 Choice",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#evaluation",
    "href": "ch6.html#evaluation",
    "title": "6  Sampling",
    "section": "6.7 Evaluation",
    "text": "6.7 Evaluation\nHere are some of the questions suggested by Fortin and Gagnon (2016) to critically examine the sampling method used for a study:\n\nIs the target population well defined?\nAre the inclusion and exclusion criteria clearly established?\nIs the sampling method probabilistic or non-probabilistic?\nWhat sampling technique was used to recruit participants?\nIs the sample representative of the target population?\nIs the sample size adequate and justified?\nIs the sample clearly described?",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#test",
    "href": "ch6.html#test",
    "title": "6  Sampling",
    "section": "6.9 Test",
    "text": "6.9 Test\n\nNumeric questions How much is 2+3? \nMultiple Choice Which is the capital city of Barbados? BridgetownGeorgetownKingstonBridgerton\nTrue or False Quarto is very cool TRUEFALSE.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#objectives",
    "href": "ch6.html#objectives",
    "title": "6  Sampling",
    "section": "",
    "text": "Define the key concepts related to sampling.\nUnderstand the different sampling techniques.\nChoose an appropriate sampling technique for a study.\nDetermine an appropriate sample size for a study.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#issues-with-sampling",
    "href": "ch6.html#issues-with-sampling",
    "title": "6  Sampling",
    "section": "6.3 Issues with sampling",
    "text": "6.3 Issues with sampling\n\n6.3.1 Sampling error\n\n\n6.3.2 Sampling bias",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#probability-sampling-methods",
    "href": "ch6.html#probability-sampling-methods",
    "title": "6  Sampling",
    "section": "6.3 Probability sampling methods",
    "text": "6.3 Probability sampling methods\nA sampling methods is probabilistic when every unit of the population has an equal chance of being included in the sample. When that is not the case, the sample method is non-probabilistic.\n\n\n\nProbability sampling methods. Source: (“Probability Sampling” 2023)\n\n\n\n6.3.1 Simple random sampling\nSimple random sampling draws random units from the entire accessible population. The process is as follows:\n\nCreate a list of all units in the accessible population.\nAssign each unit a number.\nUse a random number generator (e.g., https://www.randomizer.org/) to create a random set of n numbers within the range of your list, where n is the desired sample size.\nThe units associated with these numbers constitute your random sample.\n\n\n\n6.3.2 Systematic random sampling\nThis method is very similar to the simple random sampling, except that the units are ordered in the list, then each nth unit is selected, starting with a randomly selected position in the list. The process is as follows:\n\nCreate a list of all units in the accessible population.\nOrder the units in some way (e.g., alphabetical order).\nAssign each unit a number.\nUse a random number generator (e.g., https://www.randomizer.org/) to generate a single random number within the range of numbers from your list.\nThis number is the first chosen unit for your sample.\nDetermine an appropriate interval to obtain a sample of the desired size. (population size/desired sample size = interval; ex. 1000/100 = 10)\nStarting from the unit you selected at step #5, move down your list and select each nth where n is your interval.\nGo back to the beginning of the list if you reach the end.\nStop when you have a sample of the desired size.\n\n\n\n6.3.3 Stratified random sampling\nThe stratified random sample aims to ensure that all groups within an heterogenous population are covered in the study. The process is as follows:\n\nCreate relatively homogenous groups (strata) based on relevant and known characteristics of the population.\nFor each group, assign a number from 1 to N where N is the total number of units in the group.\n\nFor proportional stratifiedrandom sampling\n\nFor each group, generate n random numbers from 1 to N where N in the size of the group and n is the desired group sample size, calculated like this:\n\n\\[\nGroupSampleSize = (Group Size/PopulationSize)*StudySampleSize\n\\]\nFor example, if a group represents 10% of your population, and the sample size for your study is 200, then you would want ro randomly select 0.10 * 200 = 20 units from that group.\nFor non-proportional stratified random sampling:\n\nFor each group, use a random number generator (e.g., https://www.randomizer.org/) to generate n random numbers from 1 to N where N in the size of the group and n is the desired sample size from that group (typically, but not necessarily, the total sample size desired for the study divided by the number of groups):\n\n\\[\nGroupSampleSize = StudySampleSize/Number ofgroups\n\\]\nFor example, if your target sample size is 100, and you have 4 groups, you would select 100/4 = 25 participants from each groups.\n\n\n6.3.4 Random cluster sampling\nThe random cluster sampling is convenient when the units are spread geographically, and when there are too many individual units to create a list to perform the above three types of random sampling. Instead of randomly selecting units, the researchers identify groups of units, and randomly selects groups rather than units. The method can layered and end with a random sampling of units. Here’s the process:\n\nIdentify groups.\nAssign a number to each group.\nGenerate a random set of numbers within the range of your list.\nIf you have more than one level of clustering, repeat steps 1 to 3 for each level.\nselect all units in the randomly selected clusters or apply one of the three sampling methods above to sample units within each of the randomly selected clusters.\n\nFor example, you could randomly select universities in the Canada (cluster), then randomly select programs within each of the selected universities (second level clusters) and then randomly selected classes withing each of the randomly selected programs (third level clusters) and then create a stratified random sample of students of students from each class.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#non-probability-sampling-methods",
    "href": "ch6.html#non-probability-sampling-methods",
    "title": "6  Sampling",
    "section": "6.4 Non-probability sampling methods",
    "text": "6.4 Non-probability sampling methods\n\n\n\nNon-probability sampling methods. Source: (“Non-Probability Sampling” 2023)\n\n\n\n6.4.1 Convenience sampling\nA sample is a convenience sample (also called accidental sample) when the units are selected based on their availability. In fact, they may not be selected at all, but based on whoever happened to show up at a specific time and place. Conveniene samples sometimes rely on ads (e.g. the ones you see sometimes on bulletin boards) to recruit participants.\n\n\n6.4.2 Purposive sampling\nIn purposive sampling the units are “cherry picked” by the researcher as a strategy to fulfill the goal of the study. There are several types of strategies:\n\nExtreme cases sampling:for example looking at the best and the worst performing players in a team, so explore factors that may explain their performance.\nHomegenous sampling: selecting very similar units can help studying a group or phenomenon with more depth (as opposed to exploring differences between units or groups).\nMaximum variation/diversity/heterogeneity sampling: is essentially the opposite of homogenous sampling, and is effective when the goal is to capture the broadest set of different perspective possible, and to explore/describe relationships between variables.\n\n\n\n6.4.3 Quota sampling\nQuota sampling is similar to the stratified random sampling, but without the randomization element. Units are chosen either through convenience or purposefully, but the researcher also aims at creating a sample that is representative of the population by selecting participants from groups based on known and relevant characteristics. Like in the stratified random sampling. The quotas can be proportional to the the size of each group, of equal size, or based on any other decision made by the researcher.\n\n\n6.4.4 Snowball sampling\nThe snowball sample starts with a few number of participants, and then the researcher asks participants to provide the names and contact information for other people who they think also meet the selection criteria for the study. This can be a very useful method when potential participants are very hard to find.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#references",
    "href": "ch6.html#references",
    "title": "6  Sampling",
    "section": "6.8 References",
    "text": "6.8 References\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates.\n\n\nFortin, Marie-Fabienne, and Johanne Gagnon. 2016. Fondements Et Étapes Du Processus de Recherche: Méthodes Quantitatives Et Qualitatives. 3e ed. Montréal, QC: Chenelière Éducation.\n\n\n“Non-Probability Sampling.” 2023. https://www.geeksforgeeks.org/non-probability-sampling/.\n\n\n“Probability Sampling.” 2023. https://www.geeksforgeeks.org/probability-sampling/.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#sample-1",
    "href": "ch6.html#sample-1",
    "title": "6  Sampling",
    "section": "6.6 Sample",
    "text": "6.6 Sample",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#selection-criteria",
    "href": "ch6.html#selection-criteria",
    "title": "6  Sampling",
    "section": "6.3 Selection criteria",
    "text": "6.3 Selection criteria",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#choosing-a-sample-method",
    "href": "ch6.html#choosing-a-sample-method",
    "title": "6  Sampling",
    "section": "6.5 Choosing a sample method",
    "text": "6.5 Choosing a sample method\nSome factors to take into account:\n\nThe population: If your population is heterogenous, probability sample tends to be better. However, it may be difficult, in some cases, to have a list of all the units in the accessible population, which is a requirement of probabilty sampling.\nThe objective/type of the study: If you are doing correlational predictive/confirmative, or experimental research, you will want to use probability sampling. However, exploratory and descriptive research tends to use more non-probability sampling methods.\nThe available time and resources: Probability sampling can require more time and resources than non-probability sampling.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#choosing-a-sample-size",
    "href": "ch6.html#choosing-a-sample-size",
    "title": "6  Sampling",
    "section": "6.6 Choosing a sample size",
    "text": "6.6 Choosing a sample size\nThe general principle is that your sample size should be large enough to give credibility to your findings. A commonly used rule of thumb is that 10% of the accessible population is usually appropriate sample size, or 25-30 participants from each group in a stratified or quota sample.\nYou can also use a sample size calculator such as this one: https://www.calculator.net/sample-size-calculator.html\nSome factors to take into account:\n\nThe type of study: Study that require statistical analyses, like correlational and experimental designs, tend to require a relatively large samples, as opposed to qualitative and exploratory studies than can have very small samples.\nThe desired statistical power: According to Cohen (1988), the minimum acceptable level of statistical power in 0.8 (meaning that there is a 80% chance that an effect/relationship is it does it). But if you aim for a statistical power of .95, .99 or .999 (as is often the case) you will need a larger sample.\nThe number of variables in your study: The more variables (especially categorical variables) you include in your study, the larger the sample needs to be.\nThe homogeneity/heterogeneity of the population: If the target population is believed to be very heterogenous the sample size will need to be larger.\nThe expected variability of the measurements: if the observed phenomenon are expected to vary a lot between units, then a larger sample size will be needed to achieve statistically significant results.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#critically-examining-sampling-methods",
    "href": "ch6.html#critically-examining-sampling-methods",
    "title": "6  Sampling",
    "section": "6.7 Critically examining sampling methods",
    "text": "6.7 Critically examining sampling methods\nHere are some of the questions suggested by Fortin and Gagnon (2016) to critically examine the sampling method used for a study:\n\nIs the target population well defined?\nAre the inclusion and exclusion criteria clearly established?\nIs the sampling method probabilistic or non-probabilistic?\nWhat sampling technique was used to recruit participants?\nIs the sample representative of the target population?\nIs the sample size adequate and justified?\nIs the sample clearly described?",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch6.html#practice",
    "href": "ch6.html#practice",
    "title": "6  Sampling",
    "section": "6.9 Practice",
    "text": "6.9 Practice",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "ch5.html#guiding-principles",
    "href": "ch5.html#guiding-principles",
    "title": "5  Research ethics",
    "section": "",
    "text": "Respect for persons\nConcern for welfare\nJustice",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#consent",
    "href": "ch5.html#consent",
    "title": "5  Research ethics",
    "section": "5.2 Consent",
    "text": "5.2 Consent",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#fairness-and-equity-in-research-participation",
    "href": "ch5.html#fairness-and-equity-in-research-participation",
    "title": "5  Research ethics",
    "section": "5.3 Fairness and equity in research participation",
    "text": "5.3 Fairness and equity in research participation",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#privacy-and-confidentiality",
    "href": "ch5.html#privacy-and-confidentiality",
    "title": "5  Research ethics",
    "section": "5.4 Privacy and confidentiality",
    "text": "5.4 Privacy and confidentiality",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#conflict-of-interest",
    "href": "ch5.html#conflict-of-interest",
    "title": "5  Research ethics",
    "section": "5.5 Conflict of interest",
    "text": "5.5 Conflict of interest",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#research-involving-the-first-nations-inuit-and-métis-peoples-of-canada",
    "href": "ch5.html#research-involving-the-first-nations-inuit-and-métis-peoples-of-canada",
    "title": "5  Research ethics",
    "section": "5.6 Research involving the First Nations, Inuit and Métis Peoples of Canada",
    "text": "5.6 Research involving the First Nations, Inuit and Métis Peoples of Canada",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch7.html#learning-objectives",
    "href": "ch7.html#learning-objectives",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "",
    "text": "Understand the concepts of operationalization and measurement.\nUnderstand the four basic scales of measurements.\nrecognize the different types of measurement errors.\nUnderstand the concepts of validity and reliability.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#defining-concepts",
    "href": "ch7.html#defining-concepts",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.2 Defining concepts",
    "text": "7.2 Defining concepts",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#operationalizing-concepts",
    "href": "ch7.html#operationalizing-concepts",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.3 Operationalizing concepts",
    "text": "7.3 Operationalizing concepts",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#measuring-concepts",
    "href": "ch7.html#measuring-concepts",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.3 Measuring concepts",
    "text": "7.3 Measuring concepts\nThe measure is the observation. It’s the value that is stored in a cell in a spreadsheet.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#section",
    "href": "ch7.html#section",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.3 ",
    "text": "7.3 \n\n7.3.0.1 \n\n\n7.3.1 Examples",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#section-1",
    "href": "ch7.html#section-1",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.9 ",
    "text": "7.9",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#section-2",
    "href": "ch7.html#section-2",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.9 ",
    "text": "7.9",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#section-3",
    "href": "ch7.html#section-3",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.10 ",
    "text": "7.10 \n\n\n\n\nMusikanski, Laura, Scott Cloutier, Erica Bejarano, Davi Briggs, Julia Colbert, Gracie Strasser, and Steven Russell. 2017. “Happiness Index Methodology.” Journal of Sustainable Social Change 9 (1). https://doi.org/10.5590/JOSC.2017.09.1.02.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch5.html#the-tcps-2---core-2022",
    "href": "ch5.html#the-tcps-2---core-2022",
    "title": "5  Research ethics",
    "section": "5.7 The TCPS 2 - CORE 2022",
    "text": "5.7 The TCPS 2 - CORE 2022",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch5.html#section",
    "href": "ch5.html#section",
    "title": "5  Research ethics",
    "section": "5.8 ",
    "text": "5.8",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Research ethics</span>"
    ]
  },
  {
    "objectID": "ch7.html#measurement-errors",
    "href": "ch7.html#measurement-errors",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.5 Measurement errors",
    "text": "7.5 Measurement errors\n\n7.5.1 Type I and type II errors\n\n\n\n\n\n\n\n\n\nNegative in reality\nPositive in reality\n\n\n\n\nNegative measurement\nTrue negative\nFalse negative (type II error)\n\n\nPositive measurement\nFalse positive (type I error)\nTrue positive\n\n\n\n\n\n7.5.2 Random errors\nRandom errors are inaccurate measurements that happen by chance. They are caused by a wide range of factors that can affect the precision of a measurement. They are the noise in the signal. Random errors happen. To ensure they do not overly affect your measure/perception, a solution (if the study design and resources allow it) is to make more measurements, possibly at different times depending on the suspected source of the noise, so that the errors average out. This is part of the reason why large enough samples are needed to achieve statistical power and produce meaningful results!\n\n\n7.5.3 Systematic errors\nSystematic errors affect the accuracy of measurements. They are not random and occur because of the faulty instrument or the inadequate use of an an instrument. For example, in bibliometrics research we use a databases that have a better coverage of scholarly outputs in journal form (so we systematically fail to measure other forms of research dissemination) published in english, in the global north and by large companies (so we systematically underestimate knowledge production, dissemination and use in some area, language, etc.). You can also think of a survey or an interview guide that asks a flawed question, or does not cover an important dimension of the phenomenon.\n\n\n7.5.4 Validity and reliability of instruments\nin construction….\n\n\n7.5.5 Reliability of instruments\nin construction….\n\n\n7.5.6 Precision and accuracy\nin construction….",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#validity-and-reliability",
    "href": "ch7.html#validity-and-reliability",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.7 Validity and reliability",
    "text": "7.7 Validity and reliability",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#precision-and-sensitivity",
    "href": "ch7.html#precision-and-sensitivity",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.8 Precision and sensitivity",
    "text": "7.8 Precision and sensitivity",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#defining-and-operationalizing-concepts",
    "href": "ch7.html#defining-and-operationalizing-concepts",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.2 Defining and operationalizing concepts",
    "text": "7.2 Defining and operationalizing concepts\nThe operationalization of concepts is the process through which an abstract concept (construct) like learning, pain, happiness, success, quality, etc. is made observable and measurable Fortin and Gagnon (2016).\n\n7.2.1 Indicators\nIn order to measure the unmeasurable, we produce proxies (or indicators): quantifiable and measurable expressions of the different dimensions of a construct.\nTable 1. Examples of operational definitions in bibliometrics.\n\n\n\n\n\n\n\nDefinition\nOperational definition (proxy)\n\n\n\n\nResearch output\nPublication counts\n\n\nResearch impact\nCitation counts\n\n\nCollaboration\nCo-authorship\n\n\nSocial impact\nMentions in the news, social media, policy, etc.\n\n\nDiscipline\nDepartment affiliation? Classification of journals?\n\n\nMultidisciplinarity\n- Number of disciplines represent in a team?\n- Number of disciplines represented in a reference list?\n\n\n\n\n\n7.2.2 Indices\nAn index, or indices is typically a measure that combines multiple indicators to capture to measure a multidimensional construct in a single measure. An example from bibliometrics is the h-index. Another example is the happyness index. The dimensions of the happiness index: (Musikanski et al. 2017)\n\nPsychological Well-Being: optimism, senses of purpose and of accomplishment;\nHealth: energy level and ability to perform everyday activities;\nTime Balance: enjoyment, feeling rushed, and sense of leisure;\nCommunity: sense of belonging, volunteerism, and sense of safety;\nSocial Support: satisfaction with friends and family, feeling loved, and feeling lonely;\nEducation, Arts, and Culture: access to cultural and educational events and diversity;\nEnvironment: access to nature, pollution, and conservation;\nGovernance: trust in government, sense of corruption, and competency;\nMaterial Well-Being: financial security and meeting basic needs; and\nWork: compensation, autonomy, and productivity.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#measurement-scales",
    "href": "ch7.html#measurement-scales",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.4 Measurement scales",
    "text": "7.4 Measurement scales\nTable 2. Summary of characteristics of the four measurement scales.\n\n\n\n\n\n\n\n\n\n\n\nLabels\nMeaningul order\nProportional intervals\nCaptures absolute value of 0\n\n\n\n\nNominal scale\nX\n\n\n\n\n\nOrdinal scale\nX\nX\n\n\n\n\nInterval scale\nX\nX\nX\n\n\n\nRatio scale\nX\nX\nX\nX\n\n\n\n\n7.4.1 Nominal scale\nThe nominal scale uses categories (labels) to classify objects, events, people, etc. The categories should be:\n\nExhaustive: it should be possible to assign all observations to one of the categories.\nMutually exclusive: no observations can be assigned to more than one category.\n\nThe nominal scale reflects qualitative differences between categories and has no quantitative value.\nA dichotomous variable (yes/no, true/false) is an example of the nominal scale, there are only two categories possible and they are mutually exclusive.\n\n\n7.4.2 Ordinal scale\nThe ordinal scale introduces the notion of order or rank to the categories. It does not measure concepts in an absolute sense but in a relative sense (relative to the other categories). A Likert scale(Likert 1932) like this is an example:\n\nStrongly disagree\nDisagree\nNeither agree nor disagree\nAgree\nStrongly agree\n\n\n\n7.4.3 Interval scale\nIn an interval scale, the categories are not only distinct and ranked, but they are represented with units of measurement (meters, grams, degrees, seconds, litres, years, pages, etc.). This is a purely quantitative scale that uses numbers that are continuous and equidistant. The values are still relative, rather than absolute, and the zero is arbitrary and does not represent the absence of a phenomenon.\n\n\n7.4.4 Ratio scale\nThe ratio (or proportion) scale has all the features of the other three but is absolute and has a true zero (the absence of the measured thing), and cannot be negative.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch7.html#references",
    "href": "ch7.html#references",
    "title": "7  Defining, operationalizing and measuring concepts",
    "section": "7.6 References",
    "text": "7.6 References\n\n\n\n\nFortin, Marie-Fabienne, and Johanne Gagnon. 2016. Fondements Et Étapes Du Processus de Recherche: Méthodes Quantitatives Et Qualitatives. 3e ed. Montréal, QC: Chenelière Éducation.\n\n\nLikert, R. 1932. “A Technique for the Measurement of Attitudes.” Archives of Psychology 22 140: 55–55.\n\n\nMusikanski, Laura, Scott Cloutier, Erica Bejarano, Davi Briggs, Julia Colbert, Gracie Strasser, and Steven Russell. 2017. “Happiness Index Methodology.” Journal of Sustainable Social Change 9 (1). https://doi.org/10.5590/JOSC.2017.09.1.02.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Defining, operationalizing and measuring concepts</span>"
    ]
  },
  {
    "objectID": "ch9.html#surveys",
    "href": "ch9.html#surveys",
    "title": "9  Quantitative data collection",
    "section": "9.2 Surveys",
    "text": "9.2 Surveys\n\n9.2.1 General principles\n\n\n9.2.2 Survey design\n\n\n9.2.3 Pitfalls\n\n9.2.3.1 Survey-level\n\n\n9.2.3.2 Question-level\n\nLeading questions\n\nDouble-barreled questions\nAmbiguous questions\nLoaded questions\nToo many options\nLack of neutral options\n\n\n\n\n\n9.2.4 Tools for creating surveys\n\n9.2.4.1 Survey Monkey\n\n\n9.2.4.2 Qualtrics\n\n\n9.2.4.3 Google and Microsoft Forms",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative data collection</span>"
    ]
  },
  {
    "objectID": "ch9.html#structured-observation",
    "href": "ch9.html#structured-observation",
    "title": "9  Quantitative data collection",
    "section": "9.3 Structured observation",
    "text": "9.3 Structured observation",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative data collection</span>"
    ]
  },
  {
    "objectID": "ch9.html#references",
    "href": "ch9.html#references",
    "title": "9  Quantitative data collection",
    "section": "9.4 References",
    "text": "9.4 References",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative data collection</span>"
    ]
  },
  {
    "objectID": "ch8.html#learning-objectives",
    "href": "ch8.html#learning-objectives",
    "title": "8  Qualitative data collection",
    "section": "",
    "text": "List some of the main qualitative data collection methods.\nChoose an appropriate qualitative data collection methods for a study.",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative data collection</span>"
    ]
  },
  {
    "objectID": "ch8.html#qualitative-data-collection-methods",
    "href": "ch8.html#qualitative-data-collection-methods",
    "title": "8  Qualitative data collection",
    "section": "8.2 Qualitative data collection methods",
    "text": "8.2 Qualitative data collection methods\n\nUnstructured observation\nUnstructured interview\nSemi-structured interview\nFocus group\nCritical incident\nJournaling\n\n\n8.2.1 Unstructured observation\n\nNon-participant observation:\nParticipant observation:\n\n\n8.2.1.1 Collecting observation data\n\n\n\n8.2.2 Interviews\n\n8.2.2.1 Preparing a semi-structured interview\n\n\n8.2.2.2 Conducting a semi-structured interview\n\n\n8.2.2.3 Transcribing a semi-structured interview\n\n\n\n8.2.3 Focus groups\n\n\n8.2.4 Critical incident\n\n\n8.2.5 Journaling",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative data collection</span>"
    ]
  },
  {
    "objectID": "ch9.html#slides",
    "href": "ch9.html#slides",
    "title": "9  Quantitative data collection",
    "section": "",
    "text": "Click here to open the slides in a new tab",
    "crumbs": [
      "Methodological phase",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative data collection</span>"
    ]
  },
  {
    "objectID": "ch10.html#coding",
    "href": "ch10.html#coding",
    "title": "10  Qualitative data analysis",
    "section": "10.2 Coding",
    "text": "10.2 Coding",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#intercoder-reliability",
    "href": "ch10.html#intercoder-reliability",
    "title": "10  Qualitative data analysis",
    "section": "10.8 Intercoder reliability",
    "text": "10.8 Intercoder reliability\nIntercoder reliability, also known as interrater reliability, is a measure of the consistency or agreement between different coders for the same data. This can be achieved by developing clear coding instructions, training coders, conducting pilot sessions to refine the coding scheme, and regularly discussing discrepancies to achieve consensus.\nThere are different statistical methods for measuring intercoder reliability, the most common being Cohen’s Kappa, Krippendorff’s Alpha, or percentage agreement. A high value for either of these indicators suggests that the coding process is reliable and that the findings are not biased by individual coder differences.\n\n10.8.1 Cohen’s Kappa\n\\[k = (p_o – p_e) / (1 – p_e)\\]\nwhere, Po is the relative observed agreement between raters and Pe is the probability of chance agreement.\nk ranges between 0 and 1 and is interpreted like this:\n\n\n\nCohen’s Kappa (k)\nDegree of agreement\n\n\n\n\n0\nNone\n\n\n0.01-0.20\nSlight\n\n\n0.21-0.40\nFair\n\n\n0.41-0.60\nModerate\n\n\n0.61-0.80\nSubstantial\n\n\n0.81-0.99\nGood\n\n\n1\nPerfect\n\n\n\n Here is an example:\n\n\n\n\n\nrater1\nrater2\nrater3\nrater4\nrater5\nrater6\n\n\n\n\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n5. Other\n5. Other\n5. Other\n\n\n2. Personality Disorder\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n5. Other\n\n\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n\n\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n1. Depression\n1. Depression\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n\n\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n5. Other\n5. Other\n\n\n1. Depression\n1. Depression\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n4. Neurosis\n\n\n1. Depression\n1. Depression\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n\n\n1. Depression\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n1. Depression\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n\n\n1. Depression\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n5. Other\n\n\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n5. Other\n\n\n1. Depression\n1. Depression\n1. Depression\n4. Neurosis\n5. Other\n5. Other\n\n\n1. Depression\n1. Depression\n1. Depression\n1. Depression\n1. Depression\n2. Personality Disorder\n\n\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n1. Depression\n3. Schizophrenia\n3. Schizophrenia\n5. Other\n5. Other\n5. Other\n\n\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n\n\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n5. Other\n5. Other\n5. Other\n\n\n1. Depression\n1. Depression\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n1. Depression\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n5. Other\n\n\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n\n\n1. Depression\n1. Depression\n1. Depression\n1. Depression\n5. Other\n5. Other\n\n\n2. Personality Disorder\n2. Personality Disorder\n4. Neurosis\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n1. Depression\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n3. Schizophrenia\n\n\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n5. Other\n\n\n\n\n\n\n\n\n\n Light's Kappa for m Raters\n\n Subjects = 30 \n   Raters = 6 \n    Kappa = 0.459 \n\n        z = 2.31 \n  p-value = 0.0211 \n\n\n\n\n10.8.2 Krippendorff’s Alpha\nWhereas Cohen’s Kappa is mainly used for nominal data, Krippendorff’s Alpha can handle various types of data, including nominal, ordinal, interval, and ratio scales. It also handles missing data better than the Cohen’s Kappa.\nHere’s an example:\n\n\n\n\n\ncoder\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\n\n\n\n\n1\n1\n2\n3\n3\n2\n1\n4\n1\n2\nNA\nNA\nNA\n\n\n2\n1\n2\n3\n3\n2\n2\n4\n1\n2\n5\nNA\nNA\n\n\n3\nNA\n3\n3\n3\n2\n3\n4\n2\n2\n5\n1\n3\n\n\n4\n1\n2\n3\n3\n2\n4\n4\n1\n2\n5\n1\nNA\n\n\n\n\n\n\n\nKrippendorf’s Alpha based on the nominal scale\n\n\n Krippendorff's alpha\n\n Subjects = 12 \n   Raters = 4 \n    alpha = 0.743 \n\n\nKrippendorf’s Alpha based on the ordinal scale\n\n\n Krippendorff's alpha\n\n Subjects = 12 \n   Raters = 4 \n    alpha = 0.815 \n\n\nKrippendorf’s Alpha based on the interval scale\n\n\n Krippendorff's alpha\n\n Subjects = 12 \n   Raters = 4 \n    alpha = 0.849 \n\n\nKrippendorf’s Alpha based on the ratio scale\n\n\n Krippendorff's alpha\n\n Subjects = 12 \n   Raters = 4 \n    alpha = 0.797 \n\n\n\n\n10.8.3 Percentage agreement\nThe percentage agreement is a more simple methods that is simply, as its name suggest, the percentage of codes for which all coders are in agreement. Here’s 20 codes from 4 different coders.\n\n\n\n\n\nrater1\nrater2\nrater3\nrater4\n\n\n\n\n4\n4\n3\n4\n\n\n4\n4\n4\n5\n\n\n4\n4\n5\n5\n\n\n4\n4\n4\n4\n\n\n4\n3\n2\n4\n\n\n4\n4\n3\n4\n\n\n4\n3\n2\n5\n\n\n4\n4\n3\n4\n\n\n4\n3\n3\n4\n\n\n4\n3\n3\n4\n\n\n4\n4\n3\n4\n\n\n4\n3\n3\n4\n\n\n4\n4\n4\n4\n\n\n4\n4\n4\n4\n\n\n4\n4\n3\n4\n\n\n4\n4\n4\n4\n\n\n4\n4\n4\n4\n\n\n4\n4\n4\n4\n\n\n4\n4\n4\n4\n\n\n4\n5\n5\n4\n\n\n\n\n\n\n\nWe can then calculate the percentage simple percentage agreement.\n\n\n Percentage agreement (Tolerance=0)\n\n Subjects = 20 \n   Raters = 4 \n  %-agree = 35 \n\n\nIf we want, we can change the tolerance level so that slight disagreement is still considered an agreement. For example, if we have a tolerance of 1, then all the cases where all coders rated 3 or 4 will be considered as agreements.\n\n\n Percentage agreement (Tolerance=1)\n\n Subjects = 20 \n   Raters = 4 \n  %-agree = 90",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#tools",
    "href": "ch10.html#tools",
    "title": "10  Qualitative data analysis",
    "section": "10.4 Tools",
    "text": "10.4 Tools\n\n10.4.1 NVIVO\nNVivo is a popular and powerful software for qualitative data analysis but it may require a significant investment of time and money to fully utilize its capabilities.\nPros:\n\nOrganized Data Analysis: NVivo provides a structured approach to organizing and analyzing qualitative data, making it easier to manage large datasets\nSupports Multiple Formats: It can handle various data formats, including text, audio, video, and social media content\nAdvanced Features: NVivo includes AI-powered autocoding and sentiment analysis, which can speed up the research process\nVisualization Tools: The software offers robust visualization options, helping to create clear and insightful graphics\nCollaboration: NVivo supports team collaboration, allowing multiple users to work on the same project simultaneously\n\nCons:\n\nSteep Learning Curve: The software can be complex and may require significant time to learn and master\nHigh Cost: NVivo can be expensive, especially for individual users or small organizations\nPerformance Issues: Some users report that the software can be slow and occasionally frustrating to use\nLimited Free Trial: NVivo offers a limited free trial, and there is no free version available\ninterface Clutter: The interface can be overwhelming for new users, making it difficult to navigate\n\n\n\n10.4.2 QDA Miner\n\n\n10.4.3 LibreQDA\nLibreQDA is a free software developed by the Plateforme en humanités numériques of the University of Sherbrooke. It is more limited than NVIVO and QDA Miner, but it’s free. According to its documentation, it allows users to :\n\nImporting a variety of text files (.pdf, .docx, .odt, .txt and more);\nCoding words, sentences or paragraphs using codes manually set by the user;\nFinding and analyzing themes of interest;\nExporting a subset of coded selections or the project as a whole.\n\nSome of its main limitations are:\n\nCan’t process images, audio or video;\nConverts all documents to a nearly raw text format, which can result in files that are difficult to read if source documents had complex formatting;\nInstantly applies changes to each project for all team members, which can influence their reading and analysis of the documents;\nDoes not provide any advanced modules to explore the co-occurence of codes, such as a summary view of the contents in a matrix.\n\n\n\n10.4.4 Excel",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#learning-objectives",
    "href": "ch10.html#learning-objectives",
    "title": "10  Qualitative data analysis",
    "section": "",
    "text": "Understand the qualitative data analysis process.\nKnow how to develop a codebook.\nKnow how to perform qualitative data coding.\nKnow how to perform an intercoder reliability test.\nKnow how to draw insights from qualitative coding.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#core-principles-of-qualitative-data-analysis",
    "href": "ch10.html#core-principles-of-qualitative-data-analysis",
    "title": "10  Qualitative data analysis",
    "section": "10.2 Core principles of qualitative data analysis",
    "text": "10.2 Core principles of qualitative data analysis\nQualitative data analysis is about creating/extracting meaning from or making sense of unstructured qualitative data such as text, images, and videos. Depending on the topic and the availability or not of a theoretical or conceptual framework to guide the analysis, you may be starting with a set of themes or concepts that you are looking to identify in the data, or you may be going in with little or no a prioris.\nThe qualitative data analysis process generally looks like this:\n\nCreating a preliminary codebook (optional).\nSegmenting the data.\nFirst cycle coding.\nExpanding/finalizing codebook.\nSecond cycle coding.\nIntercoder or intracoder reliability testing (not always necessary).\nRepeating step 3-4-5-6 until stabilization/agreement.\nDrawing conclusions.\n\nBecause the nature of the codes in the codebook will depend on the coding performed, we will first look at the processes of segmenting and coding the data.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#the-coding-process",
    "href": "ch10.html#the-coding-process",
    "title": "10  Qualitative data analysis",
    "section": "10.3 The coding process",
    "text": "10.3 The coding process\n\n10.3.1 Types of coding\nIn their book Qualitative Data Analysis: A methods sourcebook, Miles et al. Miles, Huberman, and Saldaña (2020) identify 18 different types of coding, which they divide in six categories (elemental, affective, literary, exploratory, procedural, and grammatical).\n\n10.3.1.1 Elemental methods\n\nDescriptive coding: using codes (usually nouns) that describe what the segment of data is about, its topic. (e.g., “businesses”, “books”, “activities”, “colleagues”).\nIn Vivo coding: using the words used by participants as codes.\nProcess coding: using active words (ending in “ing”) to code for processes and actions (e.g., “spending time with each other”, “listening”, “taking the lead”).\nConcept coding: codes that represent higher level meaning, which is not explicit in the data. (e.g., “existential dread”, “the American dream”)\n\n\n\n10.3.1.2 Affective methods\n\nEmotion coding: usingcodes that represent emotions recalled, felt, or discussed by the participants (e.g., “hate”, “love”, “worry”, “hope”)\nValues coding: usingcodes that represent the participant’s values (V), attitudes (A) and beliefs (B). The codes will usually distinguish between the three, using (e.g., “V: respect”, “B: hard work leads to success”, “A: Open-Mindedness”).\nEvaluation coding: adding tags to codes with + or - to represent positivity or negativity (e.g., “+ successful candidates”, “- mistakes made”, “+ lessons learned”)\n\n\n\n10.3.1.3 Literary method\n\nDramaturgical coding: using prefixes to assign categories to codes:\n\nObjectives (e.g. “OBJ: Marrying Peach”)\nConflicts (e.g., “CON: Control over Mushroom Kingdom”)\nTactics (e.g., “TAC: Kidnapping Peach”)\nAttitudes (e.g., “ATT: Courage”)\nEmotions (e.g., “EMO: Fear”)\nSubtexts (SUB): things that are implied and not explicitly stated.\n\n\n\n\n10.3.1.4 Exploratory method\n\nHolistic coding:\nProvisional coding:\nHypothesis coding:\n\n\n\n10.3.1.5 Procedural methods\n\nProtocol coding:\nCausation coding:\n\n\n\n10.3.1.6 Grammatical methods\n\nAttribute coding: codes capturing the attributes of participant, the case, the interview, context, etc.\nMagnitude coding:\nSubcoding:\nSimultaneous coding:\n\n\n\n10.3.1.7 A standalone method\n\nTheming the data:\n\n\n\n\n10.3.2 The coding process\n\n\n10.3.3 Data units\n\n\n10.3.4 Codebook\n\n\n10.3.5 Intercoder reliability\n\n\n10.3.6 Tools for coding\n\n10.3.6.1 NVIVO\nNVivo is a popular and powerful software for qualitative data analysis, but it may require a significant investment of time and money to fully utilize its capabilities.\nPros:\n\nOrganized data analysis: NVivo provides a structured approach to organizing and analyzing qualitative data, making it easier to manage large datasets\nSupports multiple formats: It can handle various data formats, including text, audio, video, and social media content\nAdvanced features: NVivo includes AI-powered autocoding and sentiment analysis, which can speed up the research process\nVisualization tools: The software offers robust visualization options, helping to create clear and insightful graphics\nCollaboration: NVivo supports team collaboration, allowing multiple users to work on the same project simultaneously\n\nCons:\n\nSteep learning curve: The software can be complex and may require significant time to learn and master\nHigh cost: NVivo can be expensive, especially for individual users or small organizations\nLimited free trial: NVivo offers a limited free trial, and there is no free version available\ninterface clutter: The interface can be overwhelming for new users, making it difficult to navigate\n\n\n\n10.3.6.2 QDA Miner\nQDA Miner is another powerful tool for qualitative data analysis, especially if you are looking for a cost-effective and user-friendly option.\nPros:\n\nUser-friendly interface: QDA Miner is known for its intuitive design, making it accessible for both novice and experienced researchers.\nVersatile data handling: It supports various data formats, including text, audio, and video, allowing for comprehensive analysis.\nIntegration with other tools: QDA Miner integrates well with other software like WordStat and SimStat, which is beneficial for mixed methods research.\nRobust reporting features: The software offers strong reporting capabilities, enabling users to generate detailed analysis reports.\nCost-effective: Compared to some other qualitative data analysis tools, QDA Miner is relatively affordable and offers a free version.\n\nCons:\n\nLimited advanced features: While it is user-friendly, QDA Miner may lack some of the advanced features found in other software like NVivo.\nLearning curve: Although it is generally user-friendly, there can still be a learning curve for those new to qualitative data analysis software.\nLimited collaboration tools: QDA Miner may not offer as robust collaboration features as some other tools, which can be a drawback for team projects.\n\n\n\n10.3.6.3 LibreQDA\nLibreQDA is a free software developed by the Plateforme en humanités numériques of the University of Sherbrooke. It is more limited than NVIVO and QDA Miner, but it’s free. According to its documentation, it allows users to :\n\nImporting a variety of text files (.pdf, .docx, .odt, .txt and more);\nCoding words, sentences or paragraphs using codes manually set by the user;\nFinding and analyzing themes of interest;\nExporting a subset of coded selections or the project as a whole.\n\nSome of its main limitations are:\n\nCan’t process images, audio or video;\nConverts all documents to a nearly raw text format, which can result in files that are difficult to read if source documents had complex formatting;\nInstantly applies changes to each project for all team members, which can influence their reading and analysis of the documents;\nDoes not provide any advanced modules to explore the co-occurence of codes, such as a summary view of the contents in a matrix.\n\n\n\n10.3.6.4 Excel",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#drawing-conclusions",
    "href": "ch10.html#drawing-conclusions",
    "title": "10  Qualitative data analysis",
    "section": "10.4 Drawing conclusions",
    "text": "10.4 Drawing conclusions",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#assessing-the-qualitative-data-analysis-process",
    "href": "ch10.html#assessing-the-qualitative-data-analysis-process",
    "title": "10  Qualitative data analysis",
    "section": "10.10 Assessing the qualitative data analysis process",
    "text": "10.10 Assessing the qualitative data analysis process\n\n10.10.1 Confirmability\n\nThe methods are clearly described.\nThe conclusions are supported by the data.\nThe researcher’s personal assumptions, values and potential biases are transparently reported.\nCompeting conclusions have been considered.\n\n\n\n10.10.2 Dependability\n\nThe research questions are clear and the methods align with them.\nThe theoretical framework is well defined.\nData was collected across various settings, times, respondents, etc.\nIntercoder reliability checks were made with good results.\n\n\n\n10.10.3 Credibility\n\nDescriptions are contextualized, rich, and meaningful.\nThe reported insights are plausible.\nThe data is linked to concepts in prior or emerging theory.\n\n\n\n10.10.4 Transferability\n\nThe sample and its characteristics are described enough to allow comparison with other samples\nThe limitations related to the sample size or composition are discussed.\nThe processes and outcomes are applicable to comparable settings.\nThe findings are connected to theory, old or new.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#exercise",
    "href": "ch10.html#exercise",
    "title": "10  Qualitative data analysis",
    "section": "10.11 Exercise",
    "text": "10.11 Exercise\nTo practice qualitative data analysis, we are going to use the TQRMUL dataset, which consists of video and audio recordings together with transcripts of five interviews with undergraduate students on the subject of friendship.\nYou can download the files here:\n\nTrevor\nAlexander\nDeborah\nLouise\nShazia\n\nDetails upcoming….",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#segmenting-the-data",
    "href": "ch10.html#segmenting-the-data",
    "title": "10  Qualitative data analysis",
    "section": "10.3 Segmenting the data",
    "text": "10.3 Segmenting the data\nThis step involves dividing your data (e.g., a text, an interview transcript) into chunks that will then be coded. The segments could be very large (an entire document, a chapter, a page, a paragraph), or very small (sentences, lines of text, single words). This choice is generally driven by the nature of your questions, methodological design, and data. For instance, if you are coding an interview in which the interviewer and the interviewees exchange questions and answers, this can offer a natural segmentation of the text and you may decide to use the participant’s answer as your segment.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#first-cycle-of-coding",
    "href": "ch10.html#first-cycle-of-coding",
    "title": "10  Qualitative data analysis",
    "section": "10.4 First cycle of coding",
    "text": "10.4 First cycle of coding\nIn their book Qualitative Data Analysis: A methods sourcebook, Miles, Huberman, and Saldaña (2020) identify 18 different types of coding, which they divide into six categories (elemental, affective, literary, exploratory, procedural, and grammatical). This may be an overwhelming number of coding methods to choose from, but in reality your coding method will likely be dictated to some degree by the topic, your research objectives, your methods, and preexisting theoretical or conceptual framework. We are listing the different coding methods here mainly to give you an idea of the flexibility of qualitative data analysis, so that you avoid locking yourself into a preconceived, narrow idea of how coding is supposed to be done.\n\n10.4.1 Elemental methods\n\nDescriptive coding: using codes (usually nouns) that describe what the segment of data is about, its topic. (e.g., “businesses”, “books”, “activities”, “colleagues”).\nIn Vivo coding: using the words used by participants as codes.\nProcess coding: using active words (ending in “ing”) to code for processes and actions (e.g., “spending time with each other”, “listening”, “taking the lead”).\nConcept coding: codes that represent higher level meaning, which is not explicit in the data. (e.g., “existential dread”, “the American dream”)\n\n\n\n10.4.2 Affective methods\n\nEmotion coding: usingcodes that represent emotions recalled, felt, or discussed by the participants (e.g., “hate”, “love”, “worry”, “hope”)\nValues coding: usingcodes that represent the participant’s values (V), attitudes (A) and beliefs (B). The codes will usually distinguish between the three, using (e.g., “V: respect”, “B: hard work leads to success”, “A: Open-Mindedness”).\nEvaluation coding: adding tags to codes with + or - to represent positivity or negativity (e.g., “+ successful candidates”, “- mistakes made”, “+ lessons learned”)\n\n\n\n10.4.3 Literary method\n\nDramaturgical coding: using prefixes to assign categories to codes:\n\nObjectives (e.g. “OBJ: Marrying Peach”)\nConflicts (e.g., “CON: Control over Mushroom Kingdom”)\nTactics (e.g., “TAC: Kidnapping Peach”)\nAttitudes (e.g., “ATT: Courage”)\nEmotions (e.g., “EMO: Fear”)\nSubtexts (SUB): things that are implied and not explicitly stated.\n\n\n\n\n10.4.4 Exploratory method\n\nHolistic coding: Applying codes to large chunks of data (as opposed to more detailed coding). Often used as a preliminary step before doing more detailed coding.\nProvisional coding: Begins with an a priori list of codes (based on previous research), which are then revised, deleted, expanded.\nHypothesis coding: Coding based on an a priori hypothesis that the researcher wants to verify using qualitative data.\n\n\n\n10.4.5 Procedural methods\n\nProtocol coding: Using a standardized coding scheme.\nCausation coding: Coding aimed at capturing causal relationships in the data. The goal is to identify combinations of codes, such that CODE 1 -&gt; CODE 2 -&gt; CODE 3.\n\n\n\n10.4.6 Grammatical methods\n\nAttribute coding: codes capturing the attributes of participant, the case, the interview, context, etc.\nMagnitude coding: Tags added to codes to signify their magnitude (e.g., MAJOR/MODERATE/MINOR, 0 = no, 1 = possibly, and 2 = clearly, or ++ = very effective, + = effective, +- = mixed).\nSubcoding: Second order tag that represents the hierarchical nature of codes (e.g., Course - design, Course - teaching, Course - evaluation).\nSimultaneous coding: When two different codes are applied to the same chunk of data.\n\n\n\n10.4.7 A standalone method\n\nTheming the data: coding with categories or themes similar to those who are employed to cluster codes in second cycle coding (see below).",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#the-codebook",
    "href": "ch10.html#the-codebook",
    "title": "10  Qualitative data analysis",
    "section": "10.6 The Codebook",
    "text": "10.6 The Codebook\n\n10.6.1 A priori codes\nIt may be useful to establish an a priori set of codes so that the researcher(s). These a priori codes should be drawn from, again, your topic, your research objectives, your methods, and preexisting theoretical or conceptual frameworks.\n\n\n10.6.2 Revising codes\nWhen doing the first cycle of coding, it’s better to create too many codes than not enough codes. It’s also best to slightly “overcode” (adding codes that might end up being dropped later) than to “undercode” (not coding some segments that contain meaningful data). This will allow you to work with a comprehensive set of codes at the revision stages. This is when codes get adjusted, refined, combined, or divided into subcodes.\n\n\n10.6.3 Defining codes\nEvery code should have a definition that differentiates it from other codes and that can help coders apply code the data. In addition to the definition, an example of a data chunk to which the code would apply should be included.\n\n\n10.6.4 Example\nHere is an example of a codebook from a study (not yet published) on librarian experience and perceptions about the process of unbundling the “big deal” (journal subscriptions from large commercial publishers). Note that ideally this codebook would have included a definition of each code, as well as an example chunk of data to which the code was applied.\n\n\n\n\n\nCodes\n\n\n\n\nDecision-making\n\n\nConfidence in decisions\n\n\nFactors involved in decision-making\n\n\nLibrarians’ involvement in decisions\n\n\nAttitude towards cancellation\n\n\nAttitude towards Big Deals\n\n\nConsultation with librarians\n\n\nConsultations with faculty\n\n\nContext\n\n\nCancellation process\n\n\nFear of faculty response\n\n\nLibrarian opposition\n\n\nMitigation\n\n\nFaculty response\n\n\nFaculty awareness of publishing situation\n\n\nConsultation with faculty\n\n\nRelationship with faculty\n\n\nNeed for behavioural change\n\n\nAlternative means of access\n\n\nOpen access\n\n\nConsultation or collaboration with other institutions\n\n\nLiaising with top university admin\n\n\nCommunications with faculty/librarians\n\n\nTeam members\n\n\nLessons learned\n\n\nStrategy\n\n\nCovid impact\n\n\nTransformative agreements\n\n\nConfidence in data/leaders\n\n\nMethodology\n\n\nAssessing value\n\n\nMeasures\n\n\nFaculty survey\n\n\nLessons learned\n\n\nAnalysis\n\n\nChallenges\n\n\nTools\n\n\nBalancing impact\n\n\nOutcome\n\n\nWork role\n\n\nRole in cancellation\n\n\nExperience\n\n\nLibrarian knowledge\n\n\nPractice\n\n\nOrganizational structure\n\n\nManagerial style",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#second-cycle-coding",
    "href": "ch10.html#second-cycle-coding",
    "title": "10  Qualitative data analysis",
    "section": "10.7 Second cycle coding",
    "text": "10.7 Second cycle coding\nIn second cycle coding, we are assigning meaning to the codes. The goal here is to cluster codes in meaningful themes that will later guide the interpretation of the data and produce answers ot the research questions. According to Miles, Huberman, and Saldaña (2020) These clusters of codes can represent:\n\nCategories or themes\nCauses of explanations\nRelationships\nConcepts of theoretical constructs\n\nThese clusters then allow us to finalize our code book.\n\n10.7.1 Finalized code book example\nAgain, this codebook would ideally have included a definition of each code, as well as an example chunk of data to which the code was applied.\n\n\n\n\n\nthemes\nCodes\n\n\n\n\nDecision making\nDecision-making\n\n\nConfidence in decisions\n\n\nFactors involved in decision-making\n\n\nLibrarians’ involvement in decisions\n\n\nAttitude towards cancellation\n\n\nAttitude towards Big Deals\n\n\nConsultation with librarians\n\n\nConsultations with faculty\n\n\nContext\n\n\nCancellation process\n\n\nFaculty response\nFear of faculty response\n\n\nLibrarian opposition\n\n\nMitigation\n\n\nFaculty response\n\n\nFaculty awareness of publishing situation\n\n\nConsultation with faculty\n\n\nRelationship with faculty\n\n\nNeed for behavioural change\n\n\nAlternative means of access\n\n\nOpen access\n\n\nStrategy\nConsultation or collaboration with other institutions\n\n\nLiaising with top university admin\n\n\nCommunications with faculty/librarians\n\n\nTeam members\n\n\nLessons learned\n\n\nStrategy\n\n\nCovid impact\n\n\nTransformative agreements\n\n\nData analysis\nConfidence in data/leaders\n\n\nMethodology\n\n\nAssessing value\n\n\nMeasures\n\n\nFaculty survey\n\n\nLessons learned\n\n\nAnalysis\n\n\nChallenges\n\n\nTools\n\n\nBalancing impact\n\n\nOutcome\n\n\nRoles and experience\nWork role\n\n\nRole in cancellation\n\n\nExperience\n\n\nLibrarian knowledge\n\n\nPractice\n\n\nOrganizational structure\n\n\nManagerial style",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#tools-for-coding",
    "href": "ch10.html#tools-for-coding",
    "title": "10  Qualitative data analysis",
    "section": "10.5 Tools for coding",
    "text": "10.5 Tools for coding\n\n10.5.0.1 NVIVO\nNVivo is a popular and powerful software for qualitative data analysis, but it may require a significant investment of time and money to fully utilize its capabilities.\nPros:\n\nOrganized data analysis: NVivo provides a structured approach to organizing and analyzing qualitative data, making it easier to manage large datasets.\nSupports multiple formats: It can handle various data formats, including text, audio, video, and social media content.\nAdvanced features: NVivo includes AI-powered autocoding and sentiment analysis, which can speed up the research process.\nVisualization tools: The software offers robust visualization options, helping to create clear and insightful graphics.\nCollaboration: NVivo supports team collaboration, allowing multiple users to work on the same project simultaneously.\n\nCons:\n\nSteep learning curve: The software can be complex and may require significant time to learn and master.\nHigh cost: NVivo can be expensive, especially for individual users or small organizations.\nLimited free trial: NVivo offers a limited free trial, and there is no free version available.\ninterface clutter: The interface can be overwhelming for new users, making it difficult to navigate.\n\n\n\n10.5.0.2 QDA Miner\nQDA Miner is another powerful tool for qualitative data analysis, especially if you are looking for a cost-effective and user-friendly option.\nPros:\n\nUser-friendly interface: QDA Miner is known for its intuitive design, making it accessible for both novice and experienced researchers.\nVersatile data handling: It supports various data formats, including text, audio, and video, allowing for comprehensive analysis.\nIntegration with other tools: QDA Miner integrates well with other software like WordStat and SimStat, which is beneficial for mixed methods research.\nRobust reporting features: The software offers strong reporting capabilities, enabling users to generate detailed analysis reports.\nCost-effective: Compared to some other qualitative data analysis tools, QDA Miner is relatively affordable and offers a free version.\n\nCons:\n\nLimited advanced features: While it is user-friendly, QDA Miner may lack some of the advanced features found in other software like NVivo.\nLearning curve: Although it is generally user-friendly, there can still be a learning curve for those new to qualitative data analysis software.\nLimited collaboration tools: QDA Miner may not offer as robust collaboration features as some other tools, which can be a drawback for team projects.\n\n\n\n10.5.0.3 LibreQDA\nLibreQDA is a free software developed by the Plateforme en humanités numériques of the University of Sherbrooke. It is more limited than NVIVO and QDA Miner, but it’s free. According to its documentation, it allows users to :\n\nImporting a variety of text files (.pdf, .docx, .odt, .txt and more);\nCoding words, sentences or paragraphs using codes manually set by the user;\nFinding and analyzing themes of interest;\nExporting a subset of coded selections or the project as a whole.\n\nSome of its main limitations are:\n\nCan’t process images, audio or video;\nConverts all documents to a nearly raw text format, which can result in files that are difficult to read if source documents had complex formatting;\nInstantly applies changes to each project for all team members, which can influence their reading and analysis of the documents;\nDoes not provide any advanced modules to explore the co-occurence of codes, such as a summary view of the contents in a matrix.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#drawing-insights",
    "href": "ch10.html#drawing-insights",
    "title": "10  Qualitative data analysis",
    "section": "10.9 Drawing insights",
    "text": "10.9 Drawing insights\nOne we have finalized the first and second cycle coding process, it is time to interpret to interpret the data and use the codes to construct meaning in order to answer the research questions and fulfill the broader objectives of the study. Miles, Huberman, and Saldaña (2020) suggest a list of tatics for generating meaning:\n\nNoting patterns, themes\nSeeing plausibility\nClustering\nMaking metaphors\nCounting\nContrasting/comparing\nSubsuming particulars into the general\nFactoring\nNoting relations\nFinding mediating variables\nBuilding a logical chain of evidence\nMaking conceptual/theoretical coherence",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch10.html#references",
    "href": "ch10.html#references",
    "title": "10  Qualitative data analysis",
    "section": "10.12 References",
    "text": "10.12 References\n\n\n\n\nMiles, Matthew B., A. M. Huberman, and Johnny Saldaña. 2020. Qualitative Data Analysis: A Methods Sourcebook. Fourth edition. Los Angeles: SAGE.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Qualitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#processing-data",
    "href": "ch11.html#processing-data",
    "title": "11  Quantitative data analysis",
    "section": "",
    "text": "Each column represents a single variable.\nEach cell contains a single value.\nEach row contains a single observation.\n\n\n\n11.1.1 Splitting columns\nLet’s take a look at a dataset that is not tidy. You can download the dataset here to practice following the steps below. There is also a video walkthrough with additional explanations at the end of the section.\n\n\n\n\n\nstudent\ngrade\n\n\n\n\nSmith, Emily\nMGMT1001, 92 (A+); MGMT2605, 86 (A); MGMT5450, 84 (A-)\n\n\nJohnson, Michael\nMGMT1001, 77 (B+); MGMT2605, 100 (A+); MGMT5450, 74 (B)\n\n\nBrown, Olivia\nMGMT1001, 86 (A); MGMT2605, 70 (B-); MGMT5450, 99 (A+)\n\n\n\n\n\n\n\nWe can see that for each course, students are listed in a single cell, and their grades as well. To make this data tidy, a first thing we might might to do is separate each grade observation into its own row. We have nine grades in total in the set, so we would expect nine rows in total.\nFirst you will want to open the Power Query Editor by selecting all of the data and clicking on Data then From Table/Range.\n\nThen I check the my table has headers, because it does.\n\nThis will open the Power Query Editor\n\nThen follow these steps to transform your data:\n\nSelect the grade column.\nIn the Transform menu, click on Split columns.\nSelect the delimiter. In this case, choose custom and then comma followed by a space: ;\nClick on Advanced options.\nSelect Split into Rows.\n\nWhen you are done with your transformations in the Power Query Editor. You close it by clicking the Close & Load button.\n\n\n\n\n\n\nstudent\ngrade\n\n\n\n\nSmith, Emily\nMGMT1001, 92 (A+)\n\n\nSmith, Emily\nMGMT2605, 86 (A)\n\n\nSmith, Emily\nMGMT5450, 84 (A-)\n\n\nJohnson, Michael\nMGMT1001, 77 (B+)\n\n\nJohnson, Michael\nMGMT2605, 100 (A+)\n\n\nJohnson, Michael\nMGMT5450, 74 (B)\n\n\nBrown, Olivia\nMGMT1001, 86 (A)\n\n\nBrown, Olivia\nMGMT2605, 70 (B-)\n\n\nBrown, Olivia\nMGMT5450, 99 (A+)\n\n\n\n\n\n\n\nWe made some progress, but now we need to separate the course code from the grades following these steps in the Power Query Editor:\n\nSelect the grade column.\nIn the Transform menu, click on Split columns.\nSelect the delimiter. In this case, choose custom and then comma followed by a space: ,\nClick on Advanced options.\nSelect Split into Columns.\n\nThe result should look like this:\n\n\n\n\n\nstudent\ncourse_code\ngrade\n\n\n\n\nSmith, Emily\nMGMT1001\n92 (A+)\n\n\nSmith, Emily\nMGMT2605\n86 (A)\n\n\nSmith, Emily\nMGMT5450\n84 (A-)\n\n\nJohnson, Michael\nMGMT1001\n77 (B+)\n\n\nJohnson, Michael\nMGMT2605\n100 (A+)\n\n\nJohnson, Michael\nMGMT5450\n74 (B)\n\n\nBrown, Olivia\nMGMT1001\n86 (A)\n\n\nBrown, Olivia\nMGMT2605\n70 (B-)\n\n\nBrown, Olivia\nMGMT5450\n99 (A+)\n\n\n\n\n\n\n\nThis is starting to look great, although we still have the issue of the numeric and letter grades being lumped together in a cell. To make this data truly tidy, we want to separate the numeric and the letter grades, following these steps in the Power Query Editor:\n\nSelect the grade column.\nIn the Transform menu, click on Split columns.\nSelect the delimiter. In this case, choose space.\nClick on Advanced options.\nSelect Split into Columns.\nGive an appropriate name to your new columns by double clicking on the current names.\n\n\n\n\n\n\nstudent\ncourse_code\ngrade_numeric\ngrade_letter\n\n\n\n\nSmith, Emily\nMGMT1001\n92\n(A+)\n\n\nSmith, Emily\nMGMT2605\n86\n(A)\n\n\nSmith, Emily\nMGMT5450\n84\n(A-)\n\n\nJohnson, Michael\nMGMT1001\n77\n(B+)\n\n\nJohnson, Michael\nMGMT2605\n100\n(A+)\n\n\nJohnson, Michael\nMGMT5450\n74\n(B)\n\n\nBrown, Olivia\nMGMT1001\n86\n(A)\n\n\nBrown, Olivia\nMGMT2605\n70\n(B-)\n\n\nBrown, Olivia\nMGMT5450\n99\n(A+)\n\n\n\n\n\n\n\nIn this case, our letter grades are written in between parentheses, so we can remove the parentheses using the following steps:\n\nSelect the column containing the letter grades\nRight click, select Replace values…\nReplace ( by nothing.\nRepeat step 1 and 2.\nReplace ) by nothing.\n\nYour result should look like this:\n\n\n\n\n\nstudent\ncourse_code\ngrade_numeric\ngrade_letter\n\n\n\n\nSmith, Emily\nMGMT1001\n92\nA+\n\n\nSmith, Emily\nMGMT2605\n86\nA\n\n\nSmith, Emily\nMGMT5450\n84\nA-\n\n\nJohnson, Michael\nMGMT1001\n77\nB+\n\n\nJohnson, Michael\nMGMT2605\n100\nA+\n\n\nJohnson, Michael\nMGMT5450\n74\nB\n\n\nBrown, Olivia\nMGMT1001\n86\nA\n\n\nBrown, Olivia\nMGMT2605\n70\nB-\n\n\nBrown, Olivia\nMGMT5450\n99\nA+\n\n\n\n\n\n\n\nFinally, click on close and load in the home menu of the Power Query Editor. That’s it, now we have a tidy data set of grades!\n\n11.1.1.1 Video walkthrough\n\n\n\n\n\n11.1.2 Pivot and unpivot columns\nFor some reason, you may encounter datasets in matrix form where a group of columns are in fact different observations of a same variable (in this case, three course codes). You can download the example dataset here, which we use in the steps below. Again you will also find a video demo with more explanations at the end of the section.\nThe dataset looks like this:\n\n\n\n\n\nstudent\nMGMT1001\nMGMT2605\nMGMT5450\n\n\n\n\nSmith, Emily\n92\n86\n84\n\n\nJohnson, Michael\n77\n100\n74\n\n\nBrown, Olivia\n86\n70\n99\n\n\n\n\n\n\n\n\nUnpivot columns\nThe unpivot functions can be used to create a new variable (a new column) for which the values will be the three course codes. This can be done in just a few clicks:\n\nSelect the three columns that have course codes as headers.\nIn the Transform menu, click on Unpivot columns.\nDouble click on the new attributes column to rename it to course_code.\nDouble click on the new values column to rename it to grade,\n\nThe resulting table should look like this:\n\n\n\n\n\nstudent\ncourse_code\ngrade\n\n\n\n\nSmith, Emily\nMGMT1001\n92\n\n\nSmith, Emily\nMGMT2605\n86\n\n\nSmith, Emily\nMGMT5450\n84\n\n\nJohnson, Michael\nMGMT1001\n77\n\n\nJohnson, Michael\nMGMT2605\n100\n\n\nJohnson, Michael\nMGMT5450\n74\n\n\nBrown, Olivia\nMGMT1001\n86\n\n\nBrown, Olivia\nMGMT2605\n70\n\n\nBrown, Olivia\nMGMT5450\n99\n\n\n\n\n\n\n\nThat’s it, we’ve made the dataset tidy again!\n\n\npivot a column\nIf, for some reason, one wished to do the opposite operation and create multiple columns containing each possible value of a variable. This can be done with (you guessed it) the pivot function in the Power Query Editor.\n\nSelect the column to pivot.\nIn the Transform menu, click on pivot column.\nSelect which column contains the values for the new columns (the grade column in this case)\nUnder Avanced options, select Don’t aggregate.\n\nThe resulting table should look like the original dataset:\n\n\n\n\n\nstudent\nMGMT1001\nMGMT2605\nMGMT5450\n\n\n\n\nSmith, Emily\n92\n86\n84\n\n\nJohnson, Michael\n77\n100\n74\n\n\nBrown, Olivia\n86\n70\n99\n\n\n\n\n\n\n\n\n\nVideo demo\n\n\n\n\n\n11.1.3 Combining datasets\n\n\n\n\n\n\nCaution\n\n\n\nThe steps decribed below for combining datasets are not done in the Power Query Editor like most of the transformation above. You need to perform these operations directly in your Excel spreadsheet.\n\n\nSometimes the data will come in separate files so you will have to combine the pieces into a single and tidy dataset. If your two datasets contains parts of the same observations in the same order, you may be able to simply copy and paste columns from one dataset into the other. Similarly, if the two datasets have the same columns, you may be able to copy and paste rows from one dataset into the other. However, life is not always that easy, and sometimes you need to look up information about a particular entry in your dataset in another dataset with a different structure. Take the following two tables, for example.\n\nWe have on the one hand a table of grades that students received in three different courses, and on the other end a table of course details. If we want to determine, for example, the average grade in Fall or Winter semester course, we need to bring these columns into the grades dataset. In the section below, you will learn how to use the VLOOKUP function to perform this task.\n\nThe VLOOKUP() function\nThe VLOOPKUP() function can be used to combine the columns of two datasets that do not necessarily share the same structure. It looks like this:\n=VLOOKUP(lookup_value, table_array, column, col_index_num, [range_lookup])\n\nThe lookup value can be a single value, like “MGMT1001” or the coordinates of a cell that contains the value like B2.\nThe table array is the group of rows and columns (the range) in which you want to look for the lookup value. In the example above, the range would be A2:E5, A2 being the upper left corner of the range (we don’t need to include the column names), and E5 the bottom right corner of the range. However, you should always make sure that every element of your range coordinates is preceded with a dollar sign, like this: $G$2:$J$5 This fixes the range to ensure that it is not automatically modified when you copy and paste your formula to look up different values.\nThe column index number is the number of the column in the table array that contain the values you want to bring into the other dataset. For example, in the range above, the value I’m interested in is the third column of the range, so 3 is the value I need to include in this part of the formula.\nThe range lookup can take two values: TRUE or FALSE. For the purpose of this course, you should always use FALSE. TRUE is used when you want to determine whether the lookup value falls within a range of values in the table array (e.g., look up if 92 is between 90 and 100 and is a A+).\n\nPutting it all together, what we get =VLOOKUP(B2,$G$2:$J$5,3,FALSE).\nWe need to select the cell where we want the semester information to be added, and then insert or formula in the cell or in the box above.\n\nThe result should be:\n\nAnd then we double click the bottom right corner of the cell to copy the formula over the entire column, or just copy and paste it, which gives us the following result:\n\nThe following video demonstrates how to use VLOOKUP() function using the same example we just when through.\n\n\n\n\n\n\n\n\nBeware of complex relationships in the data\n\n\n\nThe examples we went through went relatively smoothly because our grade data did not contain complex relationships. Data will often complex relationships (e.g. a course taught by two instructors) or, which require special attention if we don’t want to produce valid results when we analyze the data. The following video demonstrates how this can be an issue.\n\n\nThe important thing to remember here is that you will often have to construct multiple tables from the same data in order to answer different questions.\n\n\n\n\nExercise\nIn Brightspace, under content and then data, you will find a dataset of publications authored by Dalhousie researchers, from 2023. The following four challenges will allow you to practice the skills that you have now learned:\n\nPrepare a table with the top 15 most frequent topics covered in the dataset. Topics are separated by a semi-colon (do not further split keywords in their subparts using the comma as a delimiter).\nPrepare a table with the top 10 authors in the dataset with the highest number of publications and paste it below. The names should be in the last name, first name format and must not include the author ID (the number in parentheses included after each name). Hint: When removing the author ID, remember that you can split columns only at the first or last instance of the chosen delimiter. There’s also an easy way to remove the author IDs using the Find and replace function outside the Power Query Editor.\nPrepare a table with the top 10 countries, other than Canada, with the largest publications in the dataset. Beware of the fact that a publication can be co-authored by multiple authors with affiliations in the same country. You only want to count the country once for each paper. For example, if a paper was co-authored by researchers at Dal (Canada), Harvard (US) and Berkeley (US), we want this paper to count as one paper from the US (not two). This means that you will need to keep only relevant columns in your table and remove duplicates before you create your pivot table.\nPrepare a table with the number of publications by discipline, ranked from the discipline with the most to the discipline with the least.  You will notice that the discipline column of the Dal 2023 publications table is empty. You will need to look at the journal in the sheet of the Excel file named “Journal Classification” to obtain the discipline of each paper.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#exercise",
    "href": "ch11.html#exercise",
    "title": "11  Quantitative data analysis",
    "section": "11.2 Exercise",
    "text": "11.2 Exercise\nIn Brightspace, under content and then data, you will find a dataset of publications authored by Dalhousie researchers, from 2023. The following four challenges will allow you to practice the skills that you have now learned:\n\nPrepare a table with the top 15 most frequent topics covered in the dataset. Topics are separated by a semi-colon (do not further split keywords in their subparts using the comma as a delimiter).\nPrepare a table with the top 10 authors in the dataset with the highest number of publications and paste it below. The names should be in the last name, first name format and must not include the author ID (the number in parentheses included after each name). Hint: When removing the author ID, remember that you can split columns only at the first or last instance of the chosen delimiter. There’s also an easy way to remove the author IDs using the Find and replace function outside the Power Query Editor.\nPrepare a table with the top 10 countries, other than Canada, with the largest publications in the dataset. Beware of the fact that a publication can be co-authored by multiple authors with affiliations in the same country. You only want to count the country once for each paper. For example, if a paper was co-authored by researchers at Dal (Canada), Harvard (US) and Berkeley (US), we want this paper to count as one paper from the US (not two). This means that you will need to keep only relevant columns in your table and remove duplicates before you create your pivot table.\nPrepare a table with the number of publications by discipline, ranked from the discipline with the most to the discipline with the least.  You will notice that the discipline column of the Dal 2023 publications table is empty. You will need to look at the journal in the sheet of the Excel file named “Journal Classification” to obtain the discipline of each paper.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#analyzing-categorical-data",
    "href": "ch11.html#analyzing-categorical-data",
    "title": "11  Quantitative data analysis",
    "section": "11.2 Analyzing categorical data",
    "text": "11.2 Analyzing categorical data\nCategorical variables are groups or categories that can be nominal or ordinal.\n\n11.2.1 Nominal data\nNominal variables represent categories or groups between which there is no logical or hierarchical relationship.\n\n\n11.2.2 Ordinal data\nOrdinal variables represent categories or groups that have a logical order. They are often used to transform numerical variables.\n\n\n\n\n\n\nCategories represented with numbers\n\n\n\nIt is important to look at your data to understand what the values represent. Sometimes you may have groups that are represented with numbers. When deciding what type of statistical analysis is adequate for a given variable, you most likely will want to consider treating those variables as categorical and not numerical..\n\n\n\n\n11.2.3 Example: Titanic passengers dataset\nThe following examples and videos are using a simplified version (available here) of the titanic dataset (available here). This is what our practice dataset looks like:\n\n\n\nTable 1. Titanic passengers (10 random rows shown))\n\n\npclass\nsurvived\nname\nsex\nage\nage.group\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n1\nAppleton, Mrs. Edward Dale (Charlotte Lamson)\nfemale\n53\n36 to 59\n11769\n51.4792\nC101\nSouthampton\n\n\n1\n0\nArtagaveytia, Mr. Ramon\nmale\n71\n60+\nPC 17609\n49.5042\nNA\nCherbourg\n\n\n1\n0\nAstor, Col. John Jacob\nmale\n47\n36 to 59\nPC 17757\n227.5250\nC62 C64\nCherbourg\n\n\n1\n1\nAstor, Mrs. John Jacob (Madeleine Talmadge Force)\nfemale\n18\n18 to 35\nPC 17757\n227.5250\nC62 C64\nCherbourg\n\n\n2\n0\nLamb, Mr. John Joseph\nmale\nNA\nUnknown\n240261\n10.7083\nNA\nQueenstown\n\n\n2\n1\nLaroche, Miss. Louise\nfemale\n1\n0 to 17\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n2\n1\nLaroche, Miss. Simonne Marie Anne Andree\nfemale\n3\n0 to 17\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n2\n0\nLaroche, Mr. Joseph Philippe Lemercier\nmale\n25\n18 to 35\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n3\n0\nBarry, Miss. Julia\nfemale\n27\n18 to 35\n330844\n7.8792\nNA\nQueenstown\n\n\n3\n0\nBarton, Mr. David John\nmale\n22\n18 to 35\n324669\n8.0500\nNA\nSouthampton\n\n\n3\n0\nBeavan, Mr. William Thomas\nmale\n19\n18 to 35\n323951\n8.0500\nNA\nSouthampton\n\n\n\n\n\n\n\n\n\n11.2.4 Identifying categorical data\nIn this chapter, we are focusing on categorical data, so the first step is determining which columns in our dataset contain categorical data and what type of categorical data (nominal or ordinal) they are. In the following video, I walk you through this process:\n\n\n\n\n11.2.5 Summarizing categorical data\nThere is not a lot that you can do with a single categorical variable other than reporting the frequency (count) and relative frequency (percentage) of observations for each category. To do this, we are going to use a tool that we have already learned: pivot tables.\n\n11.2.5.1 Frequency\nA typical tool to analyze categorical data is to count the number of observation that fall into each of the groups.\n\n\n\nFrequency of passengers by port of embarkation\n\n\nembarked\nfreq\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\nNA\n2\n\n\n\n\n\n\n\n\n\n11.2.5.2 Relative frequency\nThe relative frequency is simply the frequency represented as a percentage rather than count. It is obtained by first computing the frequency and then calculating the relative frequency by dividing each count by the total.\n\n\n\nrelative frequency of passengers by port of embarkation\n\n\nembarked\nfreq\nrel_freq\n\n\n\n\nCherbourg\n270\n0.2062643\n\n\nQueenstown\n123\n0.0939649\n\n\nSouthampton\n914\n0.6982429\n\n\nNA\n2\n0.0015279\n\n\n\n\n\n\n\n\n11.2.5.2.1 Rounding the values\nWhen we calculate the relative frequency, we obtain numbers with a lot of decimals. We can remove or add decimals by selecting our data and clicking on the or  buttons, respectively.\n\n\n11.2.5.2.2 Converting the relative frequency to percentages\nAnother thing we might want to do is show the relative frequency as a percentage. This can be done by clicking the % button in excel.\n\n\n\n\nrelative frequency of passengers by port of embarkation\n\n\nembarked\nfreq\nrel_freq (%)\n\n\n\n\nCherbourg\n270\n20.6\n\n\nQueenstown\n123\n9.4\n\n\nSouthampton\n914\n69.8\n\n\nNA\n2\n0.2\n\n\n\n\n\n\n\n\n\n\n11.2.5.3 Demo\nIn this video, I show you how to summarize categorical data using the frequency and the relative frequency.\n\n\n\n\n\n11.2.6 Converting numerical data into categorical data\nJust because a column in a dataset does not contain categorical data, doesn’t mean it can’t be transformed into categorical data. For examples, age groups (e.g. 0-17, 18-35, etc.) are groups (categorical) based on age (numerical). Here’s a video showing we can make that transformation in our Titanic dataset.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#analyzing-numerical-data",
    "href": "ch11.html#analyzing-numerical-data",
    "title": "11  Quantitative data analysis",
    "section": "11.3 Analyzing numerical data",
    "text": "11.3 Analyzing numerical data\nWhile summarizing categorical data is done with frequency and relative frequency tables, numerical data can be summarized using descriptive statistics divided into three groups:\n\nMeasures of central tendency.\nMeasures of dispersion.\nMeasures of skewness.\n\nIn this chapter, we will focus on only the first two: measures of central tendency and measures of dispersion.\n\n11.3.1 Measures of central tendency\nMeasures of central tendency help you summarize data by looking at the central point within it. While the mode (the value(s) with the highest frequency) is also considered a measure of central tendency, in this chapter we will only consider two: the average (or mean), and the median.\n\n\n\n\n\n\n\n\n\nStatistic\ndescription\nformula\nExcel function\n\n\n\n\nAverage or Mean\nThe sum of values divided by the number of observations\n\\[ \\overline{X}  = \\frac{\\sum{X}}{n} \\]\n=AVERAGE(x:x)\n\n\nMedian\nthe middle value of a set of number once sorted in ascending or descending order\nIf n is odd:\n\\[ M_x = x_\\frac{n + 1}{2} \\]\nIf n is even:\n\\[ M_x = \\frac{x_{(n/2)} + x_{(n/2)+1}}{2} \\]\n=MEDIAN(x:x)\n=PERCENTILE(x:x,0.50)\n=QUARTILE(x:x,2)\n\n\n\nTable 8.1. Measures of central tendency\n\n11.3.1.1 Demo\nIn the following video, I explain the how to manually calculate the average and median in a set of numerical data.\n\n\n\n\n\n11.3.2 Measures of dispersion\n\n\n\n\n\n\n\n\n\nStatistic\nDefinition\nFormula\nExcel formula\n\n\n\n\nVariance (Var)\nExpected squared deviation from the mean. Measures how far numbers spread around the average\n\\[ Var = \\frac{\\sum{(x_i-\\overline{x})^2}}{N} \\]\n=VAR(x:x)\n\n\nStandard deviation (SD)\nSquare root of the Variance.\n\\[ SD = \\sqrt{\\frac{\\sum{(x_i-\\overline{x})^2}}{N}} \\]\n=STDEV(x)\n\n\nRange\nDifference between the minimum and maximum values in the data.\nN/A\nMinimum value: =MIN(x:x)\nMaximum value: =MAX(x:x)\nRange: =MAX(x:x)-MIN(x:x)\n\n\nInterquartile range (IQR)\nThe difference between the first quartile (Q1) and the third quartile (Q3). It measures the spread of the middle 50% of the data.\nN/A\nQ1: =QUARTILE(x:x,1)\nQ3: =QUARTILE(x:x,3)\nIQR: =Q3-Q1\n\n\n\nTable 8.2. Measures of dispersion\n\n11.3.2.1 Demo\nIn the following video, I explain the how to manually calculate the differente measures of dispersion in a set of numerical data.\n\n\n\n\n\n11.3.3 Creating a descriptive statistics summary\nUsing the measures of central tendency and dispersion listed above, we can construct a descriptive statistics summary for any number of numerical variables in a dataset. Note that these summaries don’t typically display the interquartile range (IQR) and the range, but instead provide the actual values require to calculate them (i.e., Min, Q1, Q3, Max).\n\n\n\nTable 8.3. Descriptive statistics summary of age and fare in the Titanic dataset.\n\n\nvariable\nN\nMean\nSD\nVAR\nQ1\nMedian\nQ3\nMin\nMax\n\n\n\n\nage\n1046\n29.898\n14.415\n207.791\n21.000\n28.000\n39.000\n0\n80.000\n\n\nfare\n1308\n33.295\n51.759\n2678.960\n7.896\n14.454\n31.275\n0\n512.329\n\n\n\n\n\n\n\n\n11.3.3.1 Demo\nIn the following video, I show you how to create descriptive statistics summary in Excel.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#visualizing-data",
    "href": "ch11.html#visualizing-data",
    "title": "11  Quantitative data analysis",
    "section": "11.3 Visualizing data",
    "text": "11.3 Visualizing data\nIf you are interested in a deep dive in data visualization that goes well beyond the scope of this course, I recommend the Fundamentals of Data Visualization book by Claus O. Wilke available here. This specific chapter of the Wilke book will give you some information on how to format tables and figures. Here is a video showing how to paste your tables and figures in Word from your Excel, as well as how to make format them appropriately.\n\n\n\nIdentifying the data and the purpose of the visualization\nData visualizations can be fun and colorful, but remember that their most important feature is their informativeness, and that not all data can or should be visualized. Often times, not producing a visualization is the best way to go. So, before you start thinking about how to visualize your data, the first thing you will want to do is think about the purpose: What do you want to visualize, and why? Asking yourself these important questions will save you a lot of time and unnecessary suffering down the road, by preventing you from embarking on a journey to fix a problem that is not a problem and that does not need fixing.\n\n\nChoosing the right visualization method\nWhile there is rarely a single best way to visualize a specific variable, there are usually not so many options that make sense. Your main constraint is the type of data that you are working with, so that is the first question you have to answer before thinking about how it can be visualized. In fact, these are the same questions that you had to ask yourself in Chapter 7 and Chapter 8, where you learned to identify and summarize categorical and numerical data. In the next few sections, we explore the types of visualizations that are recommended for each type of data.\n\n\n11.3.1 Visualizing a categorical variable\nIf you are working with a categorical variable, such as the port of embarkation of the Titanic passengers. The first thing you need to do is summarize the data by calculating the frequency (counts) and/or relative frequency (percentages), just like you learned in Chapter 7. Once you have summarized your data, you can think about how to visualize that summary.\n\n11.3.1.1 Visualizing frequencies\nThe most common and best way to visualize frequencies is the bar chart (or column chart in Excel). Like this one:\n\nYou can also do an horizontal bar chart, like this one:\n\nIf you are working with a nominal variable then you should order the categories from either highest count to lowest count, or the other way around, as in the examples above. However, if you are working with a ordinal variable (i.e., the categories have a logical order), then you should keep that logical order for your visualization. For example, you would not want to reorder the passenger classes when visualizing the number of passengers in each class, but keep the logical order of the classes, like this:\n\n\n\n11.3.1.2 Visualizing relative frequencies\nYou can think of a percentages as a parts of a pie, the whole pie being 100%, so a popular although sometimes criticized way to visualize percentages is the pie chart.\n\nYou can also use bar charts (horizontal or vertical) to visualize relative frequencies, like this:\n\n\n\n11.3.1.3 Demo: bar charts and pie charts\n\n\n\n\n\n11.3.2 Visualizing a numerical variable\n\nHistogram\n\n\n\nBox plot\nThe box plot is a visual representation of the descriptive statistics summary that you learned to do in Chapter 8. Here is our summary of the Age variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nVar\nMin\nQ1\nMedian\nQ3\nMax\n\n\n\n\nAge\n1046\n29.9\n14.4\n207.8\n0\n21\n28\n39\n80\n\n\n\nThe box plot below uses the first quartile (Q1) and the third quartile (Q3) to create a box, with a line inside the box representing the median. The mean is represented by an X, and the whiskers show the range of the data outside the box, and the outliers (extreme values) are show as dots outside of the whiskers. There are different acceptable methods to locate the whiskers, the simplest one is to use the min and the max, other common practices include placing them at 1.5 times the interquartile range (IQR) from the nearest quartile, at one standard deviation above and below the mean, or at the 2nd and 98th percentiles. As I am writing this, I still do not know how Excel determines where to put the whiskers, but it seems close to the 99th percentile and the 1st, this is just a guess at that point as my investigations have not been fruitful.\n\n\n\nDemo: histograms and box plots\n\n\n\n\n\n11.3.3 Visualizing multiple variables\nJust like in the previous chapter, the choices available to you for visualizing the relationship between two or more variables are are strictly determined by the type of data your are working with. Basically, there are three possible combinations\n\nTwo or more categorical variables\nTwo or more numerical variables\nA combination of categorical and numerical variables\n\nHere we treat timelines as a case of their own, so that would make four different possibilities.\nThe section below provides examples of visualizations and at the end of the chapter you will find a series of video demonstration how to produce these graphs in Excel.\n\n\n11.3.4 Two categorical variables\nWhen you have two categorical variables, the process is very similar as for single categorical variables. We need to create a contigency table, as we have learned in Chapter 7, and then we have a series of choices available to us.\n\nSide-by-side bars\nThe side-by-side bars have the benefit of being very clear and allow us to display the data label (this should be avoided, however, when there are two many bars and the numbers start getting to close to each other or overlapping. However, side-by-side bars can be less space efficient than stacked bars (see below) when dealing with categories with many groups.\n\n\n\nStacked bars\nStacking the bars can a nice option also. These graphs are more space efficient, but the data labels can be harder to read. The labels can also get crowded and start overlapping when categories have small number of observations.\nOne issue with stacked bars when we do not use data labels is that it becomes difficult to see which differences between the size of two bars that are on top of the others (in this case the difference between male passengers in the first and second classes would be hard to see if the number was not included).\n\n\n\n100% stacked bars\n100% stacked bars actually allow you to reshape the bars by shaping them based on percentages, while still allowing you to show the count as the data label (unless you are working with aggregated data already in the form of percentages, then the data label would also show percentages).\n\n\n\nAdding a third variable\nSometimes, you may wish to add a third categorical variable in the mix. This is possible, as in the example below showing, for each passenger class, the number of passenger of each sex that survived or died.\n\nHere again, we can use stacked bars, however we can see that in some cases the numbers are small and overlap with the axes and are a bit harder to read. This is still a nice way to visualize the relationship between the three categorical variables (sex, survived, and passenger class).\n\n\nDemo\n\n\n\n\n\n\n11.3.5 Two numerical variables\n\nScatterplot\nThe go to graph when dealing with two numerical variables is the scatterplot, which displays each observation as a dot on the graph situated at coordinates determined by the two numerical variables. In the example below, we plotted the relationship between age and ticket fare. We also added a linear trend line and the R2 to help determine the direction and strength of the relationship between the two. Since the red line has a positive slope (it goes up as age and ticket fare increase, we can quickly see that the relationship is positive, and the low R2 value of 0.0318 tells confirms that the relationship is not strong. This is generally teh case when the dots do not seem to follow a regular pattern and are scattered all over and away from the trend line.\n\n\nDemo\n\n\n\n\n\n\n11.3.6 A numerical and a categorical variables\nWhen dealing with a numerical and a categorical variables, we have two choices… we can produce a panel of histograms (one for each possible value of the categorical variable) or make a graph with multiple box plots (one for each value of the categorical variable).\nThe example below show a panel of three histograms (one for each passenger class) with the age distribution of Titanic passengers. This works pretty well, although it’s hard to see if there is a difference between the distribution for the second and third class. Another issue is that if that if we are working with a categorical variable with a lot of possible values, the amount of space needed to visualize each distribution may quickly become an issue.\n\n\nDemo\n\n\n\n\nBox plots series\nThe two problems with histogram panels (the amount of space they take and the challenge in seeing differences) make the series of box plots an interesting alternative. As you can see, the example below is much more space efficient, and we can clearly see that the third class passengers tend to be younger then the second class passengers, who tend to be significantly younger than first class passengers. While they may not be the most popular visualization methods, box plots are a very clear and efficient way of visualizing distributions of a numerical variable for different groups.\n\n\nDemo\n\n\n\n\n\n\n11.3.7 Timelines\nFinally, line graphs are preferred when one of the variable is a temporal unit (a date, a year, a day, etc.). In the example below, we observe the trend of the number of publications in the Canadian Journal of Administrative Sciences over four decades.\n\nWe can add multiple trends to in the same graph too. For example, the graph below shows the average number of institutions and countries listed on the publications in the journals, which indicate an increase in interinstitutional and international collaboration in the field over time.\n\nThat’s it, now you know how approach the visualization of multiple variables at once to show the relationship between them. The section below contains a series of videos demonstrating how to construct the graphs that you have encountered in this chapter (and a few more).\n\nDemo",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#identifying-the-data-and-the-purpose-of-the-visualization",
    "href": "ch11.html#identifying-the-data-and-the-purpose-of-the-visualization",
    "title": "11  Quantitative data analysis",
    "section": "11.5 Identifying the data and the purpose of the visualization",
    "text": "11.5 Identifying the data and the purpose of the visualization\nData visualizations can be fun and colorful, but remember that their most important feature is their informativeness, and that not all data can or should be visualized. Often times, not producing a visualization is the best way to go.\nSo, before you start thinking about how to visualize your data, the first thing you will want to do is think about the purpose: What do you want to visualize, and why?\nAsking yourself these important questions will save you a lot of time and unnecessary suffering down the road, by preventing you from embarking on a journey to fix a problem that is not a problem and that does not need fixing.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#choosing-the-right-visualization-method",
    "href": "ch11.html#choosing-the-right-visualization-method",
    "title": "11  Quantitative data analysis",
    "section": "11.6 Choosing the right visualization method",
    "text": "11.6 Choosing the right visualization method\nWhile there is rarely a single best way to visualize a specific variable, there are usually not so many options that make sense. Your main constraint is the type of data that you are working with, so that is the first question you have to answer before thinking about how it can be visualized. In fact, these are the same questions that you had to ask yourself in Chapter 7 and Chapter 8, where you learned to identify and summarize categorical and numerical data. In the next few sections, we explore the types of visualizations that are recommended for each type of data.\n\n11.6.1 Visualizing a categorical variable\nIf you are working with a categorical variable, such as the port of embarkation of the Titanic passengers. The first thing you need to do is summarize the data by calculating the frequency (counts) and/or relative frequency (percentages), just like you learned in Chapter 7. Once you have summarized your data, you can think about how to visualize that summary.\n\n11.6.1.1 Visualizing frequencies\nThe most common and best way to visualize frequencies is the bar chart (or column chart in Excel). Like this one:\n\nYou can also do an horizontal bar chart, like this one:\n\nIf you are working with a nominal variable then you should order the categories from either highest count to lowest count, or the other way around, as in the examples above. However, if you are working with a ordinal variable (i.e., the categories have a logical order), then you should keep that logical order for your visualization. For example, you would not want to reorder the passenger classes when visualizing the number of passengers in each class, but keep the logical order of the classes, like this:\n\n\n\n11.6.1.2 Visualizing relative frequencies\nYou can think of a percentages as a parts of a pie, the whole pie being 100%, so a popular although sometimes criticized way to visualize percentages is the pie chart.\n\n\n11.6.1.3 \nYou can also use bar charts (horizontal or vertical) to visualize relative frequencies, like this:",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#visualizing-a-numerical-variable",
    "href": "ch11.html#visualizing-a-numerical-variable",
    "title": "11  Quantitative data analysis",
    "section": "11.7 Visualizing a numerical variable",
    "text": "11.7 Visualizing a numerical variable\n\n11.7.0.1 Histogram\n\n\n\n11.7.0.2 Box plot\nThe box plot is a visual representation of the descriptive statistics summary that you learned to do in Chapter 8. Here is our summary of the Age variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nVar\nMin\nQ1\nMedian\nQ3\nMax\n\n\n\n\nAge\n1046\n29.9\n14.4\n207.8\n0\n21\n28\n39\n80\n\n\n\nThe box plot below uses the first quartile (Q1) and the third quartile (Q3) to create a box, with a line inside the box representing the median. The mean is represented by an X, and the whiskers show the range of the data outside the box, and the outliers (extreme values) are show as dots outside of the whiskers. There are different acceptable methods to locate the whiskers, the simplest one is to use the min and the max, other common practices include placing them at 1.5 times the interquartile range (IQR) from the nearest quartile, at one standard deviation above and below the mean, or at the 2nd and 98th percentiles. As I am writing this, I still do not know how Excel determines where to put the whiskers, but it seems close to the 99th percentile and the 1st, this is just a guess at that point as my investigations have not been fruitful.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#creating-and-formatting-the-visualization",
    "href": "ch11.html#creating-and-formatting-the-visualization",
    "title": "11  Quantitative data analysis",
    "section": "11.8 Creating and formatting the visualization",
    "text": "11.8 Creating and formatting the visualization\n\n11.8.1 Bar charts and pie charts\n\n\n\n\n11.8.2 Histograms and box plots",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#visualizing-multiple-variables",
    "href": "ch11.html#visualizing-multiple-variables",
    "title": "11  Quantitative data analysis",
    "section": "11.9 Visualizing multiple variables",
    "text": "11.9 Visualizing multiple variables\n\n11.9.1 Choosing the right visualization for your data\nJust like in the previous chapter, the choices available to you for visualizing the relationship between two or more variables are are strictly determined by the type of data your are working with. Basically, there are three possible combinations\n\nTwo or more categorical variables\nTwo or more numerical variables\nA combination of categorical and numerical variables\n\nHere we treat timelines as a case of their own, so that would make four different possibilities.\nThe section below provides examples of visualizations and at the end of the chapter you will find a series of video demonstration how to produce these graphs in Excel.\n\n\n11.9.2 Two categorical variables\nWhen you have two categorical variables, the process is very similar as for single categorical variables. We need to create a contigency table, as we have learned in Chapter 7, and then we have a series of choices available to us.\n\n11.9.2.1 Side-by-side bars\nThe side-by-side bars have the benefit of being very clear and allow us to display the data label (this should be avoided, however, when there are two many bars and the numbers start getting to close to each other or overlapping. However, side-by-side bars can be less space efficient than stacked bars (see below) when dealing with categories with many groups.\n\n\n\n11.9.2.2 Stacked bars\nStacking the bars can a nice option also. These graphs are more space efficient, but the data labels can be harder to read. The labels can also get crowded and start overlapping when categories have small number of observations.\nOne issue with stacked bars when we do not use data labels is that it becomes difficult to see which differences between the size of two bars that are on top of the others (in this case the difference between male passengers in the first and second classes would be hard to see if the number was not included).\n\n\n\n11.9.2.3 100% stacked bars\n100% stacked bars actually allow you to reshape the bars by shaping them based on percentages, while still allowing you to show the count as the data label (unless you are working with aggregated data already in the form of percentages, then the data label would also show percentages).\n\n\n\n\n11.9.3 Adding a third variable\nSometimes, you may wish to add a third categorical variable in the mix. This is possible, as in the example below showing, for each passenger class, the number of passenger of each sex that survived or died.\n\nHere again, we can use stacked bars, however we can see that in some cases the numbers are small and overlap with the axes and are a bit harder to read. This is still a nice way to visualize the relationship between the three categorical variables (sex, survived, and passenger class).\n\n\n\n11.9.4 Two numerical variables\nThe go to graph when dealing with two numerical variables is the scatterplot, which displays each observation as a dot on the graph situated at coordinates determined by the two numerical variables. In the example below, we plotted the relationship between age and ticket fare. We also added a linear trend line and the R2 to help determine the direction and strength of the relationship between the two. Since the red line has a positive slope (it goes up as age and ticket fare increase, we can quickly see that the relationship is positive, and the low R2 value of 0.0318 tells confirms that the relationship is not strong. This is generally teh case when the dots do not seem to follow a regular pattern and are scattered all over and away from the trend line.\n\n\n\n11.9.5 A numerical and a categorical variables\nWhen dealing with a numerical and a categorical variables, we have two choices… we can produce a panel of histograms (one for each possible value of the categorical variable) or make a graph with multiple box plots (one for each value of the categorical variable).\nThe example below show a panel of three histograms (one for each passenger class) with the age distribution of Titanic passengers. This works pretty well, although it’s hard to see if there is a difference between the distribution for the second and third class. Another issue is that if that if we are working with a categorical variable with a lot of possible values, the amount of space needed to visualize each distribution may quickly become an issue.\n\nThe two problems with histogram panels (the amount of space they take and the challenge in seeing differences) make the series of box plots an interesting alternative. As you can see, the example below is much more space efficient, and we can clearly see that the third class passengers tend to be younger then the second class passengers, who tend to be significantly younger than first class passengers. While they may not be the most popular visualization methods, box plots are a very clear and efficient way of visualizing distributions of a numerical variable for different groups.\n\n\n\n11.9.6 Timelines\nFinally, line graphs are preferred when one of the variable is a temporal unit (a date, a year, a day, etc.). In the example below, we observe the trend of the number of publications in the Canadian Journal of Administrative Sciences over four decades.\n\nWe can add multiple trends to in the same graph too. For example, the graph below shows the average number of institutions and countries listed on the publications in the journals, which indicate an increase in interinstitutional and international collaboration in the field over time.\n\nThat’s it, now you know how approach the visualization of multiple variables at once to show the relationship between them. The section below contains a series of videos demonstrating how to construct the graphs that you have encountered in this chapter (and a few more).\n\n\n11.9.7 How to make these graphs in Excel (demo)\n\n11.9.7.1 Relationship between categorical variables (side-by-side and stacked bars)\n\n\n\n\n11.9.7.2 Relationship between numerical variables (scatterplots)\n\n\n\n\n11.9.7.3 Relationship between categorical and numerical variables (histogram panels)\n\n\n\n\n11.9.7.4 Relationship between categorical and numerical variables (box plots)\n\n\n\n\n11.9.7.5 Timelines",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#logistic-regression",
    "href": "ch11.html#logistic-regression",
    "title": "11  Quantitative data analysis",
    "section": "11.10 Logistic regression",
    "text": "11.10 Logistic regression\nRegression is a method used to determine the relationship between a dependent variable (the variable we want to predict) and one or more independent variables (the predictors available to make the prediction). There are a wide variety of regression methods, but in this course we will learn two: the logistic regression, which is used to predict a categorical dependent variable, and the linear regression, which is used to predict a continuous dependent variable.\nIn this chapter, we focus on the binomial logistic regression (we will refer to it as logistic regression or simply regression in the rest of the chapter), which means that our dependent variable is dichotomous (e.g., yes or no, pass vs fail). Ordinal logistic regression (for ordinal dependent variables) and multinominal logistic regression (for variables with more than 2 categories) are beyond the scope of the course.\n\n11.10.1 Logistic function\nLogistic regression is called this way because it fits a logistic function (an s-shaped curve) to the data to model the relationship between the predictors and a categorical outcome. More specifically, it models the probability of an outcome (the dependent categorical variable) based on the value of the independent variable. Here’s an example:\n\nThe model gives us, for each value of the independent variable (Copper content, in this example), the probability (odds) that the painting is an original. The point where the logistic curve reaches .5 (50%) on the y-axis is where the cut-off happens: the model predicts that any painting with a copper content above that point is an original.\n\n\n11.10.2 Odds\nWe can convert probabilities into odds by dividing the probability p of the one outcome by the probability of the other outcome, so because there are only two outcomes, then odds = p / 1-p. For example, say we have a bag of 10 balls, 2 red and 8 black. If we draw a ball at random, we have a 8/10 = 80% chance of drawing a black ball. The odds of drawing a black ball are thus 0.8/(1-0.8) = 0.8/0.2 = 4. There is a 4 to 1 chance that we’ll draw a black ball over a red one.\nHowever, the output of the logistic regression model is the natural logarithm of the odds: log odds = ln(p/1-p), so it is not so easily interpreted.\n\n\n11.10.3 Logistic regression example\n\n11.10.3.1 Loading and preparing data\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"https://pmongeon.github.io/info6270/files/data/titanic.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n6\n0\n3\nMoran, Mr. James\nmale\nNA\n0\n0\n330877\n8.4583\nNA\nQ\n\n\n\n\n\n\n\n\n11.10.3.1.1 Choose a set of predictors (independent variables)\nLooking at our dataset, we can identify some variables that we think might affect the probability that a passenger survived. In our model, we will choose Sex, Age, Pclass, and Fare. The SibSp and Parch variables represent, respectively, the combined number of siblings and spouses and the combined number of parents and children a passenger has on board. We will add them together to create a fifth predictor called FamilySize.\n\ndata &lt;- data %&gt;% \n  mutate(FamilySize = SibSp + Parch)\n\nWe can now remove the variables that we don’t need in our model by selecting the ones we want to keep.\n\ndata &lt;- data %&gt;% \n  select(Survived, Sex, Age, Fare, FamilySize, Pclass)\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nSurvived\nSex\nAge\nFare\nFamilySize\nPclass\n\n\n\n\n0\nmale\n22\n7.2500\n1\n3\n\n\n1\nfemale\n38\n71.2833\n1\n1\n\n\n1\nfemale\n26\n7.9250\n0\n3\n\n\n1\nfemale\n35\n53.1000\n1\n1\n\n\n0\nmale\n35\n8.0500\n0\n3\n\n\n0\nmale\nNA\n8.4583\n0\n3\n\n\n\n\n\n\n\n\n\n11.10.3.1.2 Set categorical variables as factors\nSetting categorical variables as factors is always necessary when fitting regression models in R. In our case there are three: Sex, Survived, and Pclass.\n\ndata &lt;- data %&gt;% \n  mutate(Sex = as_factor(Sex),\n         Survived = as_factor(Survived),\n         Pclass = as_factor(Pclass))\n\n\n\n11.10.3.1.3 Dealing with missing data\nWe can count the number of empty cells for each variable to see if some data is missing. We do this for each variable in the set.\n\nsum(is.na(data$Survived))\n\n[1] 0\n\nsum(is.na(data$Sex))\n\n[1] 0\n\nsum(is.na(data$Age))\n\n[1] 177\n\nsum(is.na(data$Fare))\n\n[1] 0\n\nsum(is.na(data$FamilySize))\n\n[1] 0\n\nsum(is.na(data$Pclass))\n\n[1] 0\n\n\nOnce we have identified that some columns contain missing data, we have two choices. We do nothing and these cases will be left out of the regression model, or we fill the empty cells in some way (this is called imputation). We have many missing values (177 out of 891 observations is quite large) and leaving out these observations could negatively affect the performance of our regression model. Therefore, we will assign the average age for all 177 missing age values, which is a typical imputation mechanism to replace NA values with an estimate based on the available data.\n\n# We use na.rm = TRUE otherwise mean(Age) would return NA due to the missing values.\ndata &lt;- data %&gt;% \n  mutate(Age = replace_na(Age, round(mean(Age, na.rm=TRUE),0)))\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nSurvived\nSex\nAge\nFare\nFamilySize\nPclass\n\n\n\n\n0\nmale\n22\n7.2500\n1\n3\n\n\n1\nfemale\n38\n71.2833\n1\n1\n\n\n1\nfemale\n26\n7.9250\n0\n3\n\n\n1\nfemale\n35\n53.1000\n1\n1\n\n\n0\nmale\n35\n8.0500\n0\n3\n\n\n0\nmale\n30\n8.4583\n0\n3\n\n\n\n\n\n\n\n\n\n11.10.3.1.4 Visualizing the relationships\nTo explore the relationship between variables. we can visualize the distribution of independent variable values for each value of the dependent variable. We can use box plots or violin plots for continuous independent variables and bar charts for the categorical variables. To make the process faster, let’s briefly untidy the data and use the gather() function to create key-value pairs for each observation of the dependent variable.\n\ndata_untidy &lt;- gather(data, key = \"Variable\", value = \"Value\",\n                        -Survived) # Creates key-value pairs for all columns except Survive\nhead(data_untidy) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nSurvived\nVariable\nValue\n\n\n\n\n0\nSex\nmale\n\n\n1\nSex\nfemale\n\n\n1\nSex\nfemale\n\n\n1\nSex\nfemale\n\n\n0\nSex\nmale\n\n\n0\nSex\nmale\n\n\n\n\n\n\n\nWe can now easily create box plots for all our independent variables and outcome.\n\ndata_untidy %&gt;% \n  filter(Variable != \"Pclass\" & Variable != \"Sex\") %&gt;%\n  ggplot(aes(Survived, as.numeric(Value))) +\n  facet_wrap(~ Variable, scales = \"free_y\") +\n  geom_boxplot(draw_quantiles = c(0.25, 0.5, 0.75))\n\nWarning in geom_boxplot(draw_quantiles = c(0.25, 0.5, 0.75)): Ignoring unknown\nparameters: `draw_quantiles`\n\n\n\n\n\n\n\n\n\nAnd now we make a bar chart for our categorical independent variables.\n\ndata_untidy %&gt;%\n  filter(Variable == \"Pclass\" | Variable == \"Sex\") %&gt;%\n  ggplot(aes(Value, fill = Survived)) +\n  facet_wrap(~ Variable, scales = \"free_x\") +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n11.10.3.2 Creating the model\nThe following code generates our logistic regression model using the glm() function (glm stands for general linear model). The syntax is gml(predicted variable ~ predictor1 + predictor2 + preductor3..., data, family) where data is our dataset and the family is the type of regression model we want to create. In our case, the family is binomial.\n\nmodel &lt;- glm(Survived ~ Sex + Age + Fare + FamilySize + Pclass,\n             data = data,\n             family = binomial)\n\n\n\n11.10.3.3 Model summary\nNow that we have created our model, we can look at the coefficients (estimates) which tell us about the relationship between our predictors and the predicted variable. The Pr(&gt;|z|) column represents the p-value, which determines whether the effect observed is statistically significant. It is common to use 0.05 as the threshold for statistical significance, so all the effects in our model are statistically significant (p &lt; 0.05) except for the fare (p &gt; 0.05).\n\nsummary(model)$coefficients %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n1.0377996\n0.3932615\n2.638956\n0.0083162\n\n\nSexfemale\n2.7763022\n0.1985984\n13.979481\n0.0000000\n\n\nAge\n-0.0388614\n0.0078206\n-4.969087\n0.0000007\n\n\nFare\n0.0032132\n0.0024551\n1.308796\n0.1906036\n\n\nFamilySize\n-0.2435093\n0.0676841\n-3.597733\n0.0003210\n\n\nPclass2\n-1.0021830\n0.2929527\n-3.420972\n0.0006240\n\n\nPclass3\n-2.1318527\n0.2891435\n-7.372993\n0.0000000\n\n\n\n\n\n\n\n\n\n11.10.3.4 Converting log odds to odds ratio\nAs we mentioned above, the coefficients produced by the model are log odds, which are difficult to interpret. We can convert them to odds ratio, which are easier to interpret. This can be done with the exp() function. We can now see that according to our model, female passengers were 16 times more likely to survive than male passengers.\n\nbind_rows(exp(model$coefficients)) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\n(Intercept)\nSexfemale\nAge\nFare\nFamilySize\nPclass2\nPclass3\n\n\n\n\n2.822998\n16.05953\n0.961884\n1.003218\n0.7838722\n0.3670772\n0.1186173\n\n\n\n\n\n\n\n\n\n11.10.3.5 Adding confidence intervals\nThe confidence intervals are an estimation of the precision odds ratio. In the example below, We use a 95% confidence interval which means that we are 95% of our estimated coefficients for a predictor are between the 2.5th percentile and the 97.5th percentile (the two values reported in the tables). If we were using a sample to make claims about a population, which does not really apply here due to the unique case of the titanic, we could then think of the confidence interval as indicating a 95% probability that the true coefficient for the entire population is situated in between the two values.\n\nodds_ratio &lt;- cbind(Odds_Ratio = exp(model$coefficients), \n                    exp(confint(model, level = .95)))\n\nodds_ratio %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\n\nOdds_Ratio\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n2.8229984\n1.3083312\n6.1328871\n\n\nSexfemale\n16.0595260\n10.9723363\n23.9218257\n\n\nAge\n0.9618840\n0.9469679\n0.9764876\n\n\nFare\n1.0032184\n0.9987147\n1.0086269\n\n\nFamilySize\n0.7838722\n0.6827452\n0.8907643\n\n\nPclass2\n0.3670772\n0.2060871\n0.6511201\n\n\nPclass3\n0.1186173\n0.0670531\n0.2089431\n\n\n\n\n\n\n\n\n\n11.10.3.6 Model predictions\nFirst we add to our data the probability that the passenger survived as calculated by the model.\n\ndata &lt;- tibble(data,\n               Probability = model$fitted.values)\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nSurvived\nSex\nAge\nFare\nFamilySize\nPclass\nProbability\n\n\n\n\n0\nmale\n22\n7.2500\n1\n3\n0.1025490\n\n\n1\nfemale\n38\n71.2833\n1\n1\n0.9107566\n\n\n1\nfemale\n26\n7.9250\n0\n3\n0.6675927\n\n\n1\nfemale\n35\n53.1000\n1\n1\n0.9153720\n\n\n0\nmale\n35\n8.0500\n0\n3\n0.0810373\n\n\n0\nmale\n30\n8.4583\n0\n3\n0.0968507\n\n\n\n\n\n\n\nThen we obtain the prediction by creating a new variable called prediction and setting the value to 1 if the calculated probability of survival is greater than 50% and 0 otherwise.\n\ndata &lt;- data %&gt;% \n  mutate(Prediction = if_else(Probability &gt; 0.5,1,0))\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\nSurvived\nSex\nAge\nFare\nFamilySize\nPclass\nProbability\nPrediction\n\n\n\n\n0\nmale\n22\n7.2500\n1\n3\n0.1025490\n0\n\n\n1\nfemale\n38\n71.2833\n1\n1\n0.9107566\n1\n\n\n1\nfemale\n26\n7.9250\n0\n3\n0.6675927\n1\n\n\n1\nfemale\n35\n53.1000\n1\n1\n0.9153720\n1\n\n\n0\nmale\n35\n8.0500\n0\n3\n0.0810373\n0\n\n\n0\nmale\n30\n8.4583\n0\n3\n0.0968507\n0\n\n\n\n\n\n\n\nThen we can print a table comparing the model’s prediction to the real data.\n\ntable(Survived = data$Survived, \n      Predicted = data$Prediction)\n\n        Predicted\nSurvived   0   1\n       0 476  73\n       1 101 241\n\n\nFinally, we can calculate how well our model fits the data (how good was it at predicting the independent variable) by testing, for each passenger in the dataset, whether the prediction matched the actual outcome for that passenger. Because that test gives TRUE (1) or FALSE (0) for every row of the dataset, calculating the mean of the test results gives us the percentage of correct predictions, which in our case is about 80.5%.\n\nmean(data$Survived == data$Prediction)\n\n[1] 0.8047138",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#linear-regression",
    "href": "ch11.html#linear-regression",
    "title": "11  Quantitative data analysis",
    "section": "11.11 Linear regression",
    "text": "11.11 Linear regression\nLinear regression uses a straight line to model the relationship between categorical or numerical predictors (independent variables) and a numerical predicted value (dependent variable). for a single predictor variable, the formula for the prediction is:\n\\[ y = intercept + slope × x \\]\nIn statistical terms, the same formula is written like this:\n\\[ y = β_0 + βx \\]\nAnd if we have multiple independent variables, the formula becomes:\n\\[ y = β_0 + β_1x_1 + β_2x_2 .... β_nx_n \\]\nTo determine how well the model fits the data (how well does x predict y. the linear model uses the square of the residuals (r2). The residuals are the difference between the predicted values and the real data, measured by the vertical distance between the line and the data points. Here’s a figure from (rhys2020?) to help you visualize this.\n\n\n\n\n\nWe can see in this figure that the intercept is where the line crosses the y-axis. The slope is calculated by dividing the difference in the predicted value of y by the difference in the value of x.\nWhen working with categorical predictors, the intercept is the mean value of the base category, and the slope is the difference between the means of each category. Here’s an example taken again from (rhys2020?).",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#building-a-linear-regression-model",
    "href": "ch11.html#building-a-linear-regression-model",
    "title": "11  Quantitative data analysis",
    "section": "11.12 Building a linear regression model",
    "text": "11.12 Building a linear regression model\nLet’s use on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013 to try to build a model that will predicted delayed arrival. For this we will use the flights dataset included in the nycflights13 package. We will consider the following variables in our model:\n\norigin: airport of departure (JFK, LGA, EWR)\ncarrier (we will only compare United Airlines - UA, and American Airlines - AA)\ndistance: flight distance in miles.\ndep_delay: delay of departure in minutes\narr_delay: delay of arrival in minutes (this is our independent variable)\n\n\nlibrary(nycflights13)\ndata &lt;- flights %&gt;% \n  filter(carrier %in% c(\"UA\", \"AA\")) %&gt;% \n  select(origin, carrier, distance, dep_delay, arr_delay) %&gt;% \n  mutate(origin = as_factor(origin),\n         carrier = as_factor(carrier)) %&gt;% \n  drop_na()\n\nhead(data) %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\norigin\ncarrier\ndistance\ndep_delay\narr_delay\n\n\n\n\nEWR\nUA\n1400\n2\n11\n\n\nLGA\nUA\n1416\n4\n20\n\n\nJFK\nAA\n1089\n2\n33\n\n\nEWR\nUA\n719\n-4\n12\n\n\nLGA\nAA\n733\n-2\n8\n\n\nJFK\nUA\n2475\n-2\n7",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#visualizing-the-relationship-between-the-variables",
    "href": "ch11.html#visualizing-the-relationship-between-the-variables",
    "title": "11  Quantitative data analysis",
    "section": "11.13 Visualizing the relationship between the variables",
    "text": "11.13 Visualizing the relationship between the variables\nWe can explore the relationship between our independent variables and our independent variable. How we will approach this depends on the type of independent variables we have.\n\n11.13.1 Continuous independent variables\nFor continuous independent variables, we do scatter plots with a fitted regression line. We can see in the plot below that there appears to be a linear relationship between the delay at departure and the delay at arrival (which is of course not so surprising). Here we used the stat_poly_line() and the stat_poly_eq() functions from the ggpmisc package to display the slope and the coefficient of determination (R2) of the regression line.\n\n\n\n\n\n\nThe coefficient of determination (R2)\n\n\n\nThe coefficient of determination should not be mistaken for the square of residuals, even thought they have the same notation (R2). The coefficient of determination tells us how well the regression line fits the data. It’s value ranges from 0 to 1. An R2 of 0 means that the linear regression model doesn’t predict your dependent variable any better than just using the average, and a value of 1 indicates that the model perfectly predicts the exact value of the dependent variable.\nAnother way to interpret the coefficient of determination is to consider it as a measure of the variation in the dependent variable is explained (or determined) by the model. For instance, a R2 of 0.80 indicates that 80% of the variation in the dependent variable is explained by the model.\n\n\n\ndata %&gt;%\n  ggplot() +\n  aes(dep_delay, arr_delay) + \n  stat_poly_line() +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  geom_point() + \n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can see that the the linear regression model using considering only the departure delay explains 79% of the variation in the arrival delay. That makes dep_delay a good predictor in our model. On the other hand, the flight distance explains less then 1% of the arrival delays, as we can see in the next graph. This makes distance a very weak predictor of arrival delays.\n\n11.13.1.1 Distance\n\ndata %&gt;%\n  ggplot() +\n  aes(dep_delay, distance) + \n  stat_poly_line() +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  geom_point() + \n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n11.13.2 Categorical independent variables\nTo test the linear relationship between an categorical independent variable and the dependent variable, we use the same approach: a scatterplot with geom_smooth(). However, geom_smooth does not work with factors and requires numerical data. Therefore, we need to convert our factors into numerical variables to visualize the trend line with geom_smooth().\nAlso, when dealing with factors variables with more than two categories (factors with more than 2 levels), we need to choose a base level to which we will compare each of the other levels. In other words, all our graphs should display only two categories.\n\n11.13.2.1 Carrier\nThe plot below shows that there is hardly any relationship between the carrier and the arrival delay, with the variable explaining less than 1% of the variation in arrival delays.\n\ndata %&gt;%\n  mutate(carrier = as.numeric(carrier)) %&gt;%\n  ggplot() +\n  aes(carrier, arr_delay) + \n  stat_poly_line() +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  scale_x_continuous(breaks = c(1,2)) +\n  geom_jitter(size = 1) +\n  geom_smooth(method=\"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n11.13.2.2 Origin\nSince origin has three levels (EWR, LGA and JFK), we want to plot choose a base level and compare each other level to this one. Let’s see which level represents which airport.\n\nlevels(data$origin)\n\n[1] \"EWR\" \"LGA\" \"JFK\"\n\n\nSo let’s choose level 1 (EWR) as our base and compare it with level 2 (LGA).\n\ndata %&gt;%\n  mutate(origin = as.numeric(origin)) %&gt;%\n  filter(origin %in% c(1,2)) %&gt;% \n  ggplot() +\n  aes(origin, arr_delay) + \n  stat_poly_line() +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  scale_x_continuous(breaks = c(1,2)) +\n  geom_jitter(size = 1) +\n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAnd then we compare level 1 (EWR) with level 3 (JFK).\n\ndata %&gt;%\n  mutate(origin = as.numeric(origin)) %&gt;%\n  filter(origin %in% c(1,3)) %&gt;% \n  ggplot() +\n  aes(origin, arr_delay) + \n  stat_poly_line() +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n  after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  scale_x_continuous(breaks = c(1,3)) +\n  geom_jitter(size = 1) +\n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAgain, we can see that the airport from which the flight takes off explains less than 1% of the arrival delays and is therefore a poor predictor.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#building-the-multiple-linear-regression-model",
    "href": "ch11.html#building-the-multiple-linear-regression-model",
    "title": "11  Quantitative data analysis",
    "section": "11.14 Building the multiple linear regression model",
    "text": "11.14 Building the multiple linear regression model\nThe process to build the model is the same as the one we used for the logistic regression in the previous chapter. In fact, the process is simpler here because we do not need to convert the coefficient into odds ratios to make them easier to interpret. Let’s use the lm() function to build the model that predicts delay at arrival based on the distance of the flight, the carrier and the origin (we’ll leave the delay of departure out of the model for now).\n\nmodel &lt;- lm(arr_delay ~ distance + carrier + origin, \n            data = data)\n\n\nsummary(model)$coefficients %&gt;% \n  kbl() %&gt;% \n  kable_classic()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n4.6299672\n0.3577348\n12.942455\n0.0000000\n\n\ndistance\n-0.0007208\n0.0002005\n-3.595645\n0.0003238\n\n\ncarrierAA\n-3.6699113\n0.3907133\n-9.392849\n0.0000000\n\n\noriginLGA\n-0.7238011\n0.4052653\n-1.785993\n0.0741037\n\n\noriginJFK\n1.6725192\n0.4638028\n3.606100\n0.0003110\n\n\n\n\n\n\n\nThe estimate coefficient represents the slope of the linear trend line for each predictor, so we can plug these values into our linear equation.\n\\[ ArrDelay = 4.63 - 0.00distance - 3.67AA - 0.72LGA + 1.67JFK \\]\nWe can see that most coefficients are statistically significant, which appears to indicate that they are good predictors, but let’s hold on for a minute before drawing too hasty conclusions. Look at the Adjusted R-squared (r2). It has a value of 0.001663, which is extremely small and indicate that the model explains less than 1% of the variance in delays. In other words, our model does not at all allow us to make predictions about delays. How can almost all predictors in a model be statistically significant and still be very bad predictors?\n\n\n\n\n\n\nBeware of too large sample\n\n\n\nStatistically significant predictors in a model with low predictive value mostly occur when our data set or sample is too large. What’s a good sample size? a good rule of thumb is 10% of the total observations, with at least ten observations per variable in the model but no more than 1000 observations in total. Let’s do this again with a sample of 500 observations. As shown in the code below, we can use the sample_n() function to create a sample of a specific size that will be used in the lm() function.\n\n\n\nmodel &lt;- lm(arr_delay ~ distance + carrier + origin, \n            data = sample_n(data, 500))\nsummary(model)\n\n\nCall:\nlm(formula = arr_delay ~ distance + carrier + origin, data = sample_n(data, \n    500))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.775 -21.118 -10.503   6.644 256.212 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  9.249378   4.206889   2.199   0.0284 *\ndistance    -0.004361   0.002492  -1.750   0.0808 .\ncarrierAA   -7.060319   4.803433  -1.470   0.1422  \noriginLGA    2.995931   4.964351   0.603   0.5465  \noriginJFK   -0.481764   5.767272  -0.084   0.9335  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37.97 on 495 degrees of freedom\nMultiple R-squared:  0.01327,   Adjusted R-squared:  0.005295 \nF-statistic: 1.664 on 4 and 495 DF,  p-value: 0.1571\n\n\nWe see that the model still does a terrible job at predicting arrival delays, and that none of the predictors are statistically significant (at the p &lt; 0.05 level).\n\n11.14.0.1 Adding departure delay to the model\nFinally, let’s add the departure delay to the model. We’ve seen in the figure above, in which we plotted the arrival delay against the departure delay, that our data points seemed to follow our trend line, so we can expect that adding this predictor will improve our model.\n\nmodel &lt;- lm(arr_delay ~ distance + carrier + origin + dep_delay, \n            data = sample_n(data, 500))\nsummary(model)\n\n\nCall:\nlm(formula = arr_delay ~ distance + carrier + origin + dep_delay, \n    data = sample_n(data, 500))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.972 -11.940  -1.504   9.920 110.125 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.5937564  2.0143614  -4.763 2.51e-06 ***\ndistance    -0.0005595  0.0011381  -0.492    0.623    \ncarrierAA   -0.2485397  2.1105427  -0.118    0.906    \noriginLGA    1.3649013  2.1834182   0.625    0.532    \noriginJFK    0.5839076  2.6608541   0.219    0.826    \ndep_delay    1.0549086  0.0253583  41.600  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 494 degrees of freedom\nMultiple R-squared:  0.7811,    Adjusted R-squared:  0.7788 \nF-statistic: 352.5 on 5 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see that when we consider the delay in the departure, we can more accurately predict the delay at arrival, with around 80% of the variance explained by our model!",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#analyzing-data",
    "href": "ch11.html#analyzing-data",
    "title": "11  Quantitative data analysis",
    "section": "11.2 Analyzing data",
    "text": "11.2 Analyzing data\n\n11.2.1 Analyzing categorical data\nCategorical variables are groups or categories that can be nominal or ordinal.\n\nNominal data\nNominal variables represent categories or groups between which there is no logical or hierarchical relationship.\n\n\nOrdinal data\nOrdinal variables represent categories or groups that have a logical order. They are often used to transform numerical variables.\n\n\n\n\n\n\nCategories represented with numbers\n\n\n\nIt is important to look at your data to understand what the values represent. Sometimes you may have groups that are represented with numbers. When deciding what type of statistical analysis is adequate for a given variable, you most likely will want to consider treating those variables as categorical and not numerical..\n\n\n\n\nExample: Titanic passengers dataset\nThe following examples and videos are using a simplified version (available here) of the titanic dataset (available here). This is what our practice dataset looks like:\n\n\n\nTable 1. Titanic passengers (10 random rows shown))\n\n\npclass\nsurvived\nname\nsex\nage\nage.group\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n1\nAppleton, Mrs. Edward Dale (Charlotte Lamson)\nfemale\n53\n36 to 59\n11769\n51.4792\nC101\nSouthampton\n\n\n1\n0\nArtagaveytia, Mr. Ramon\nmale\n71\n60+\nPC 17609\n49.5042\nNA\nCherbourg\n\n\n1\n0\nAstor, Col. John Jacob\nmale\n47\n36 to 59\nPC 17757\n227.5250\nC62 C64\nCherbourg\n\n\n1\n1\nAstor, Mrs. John Jacob (Madeleine Talmadge Force)\nfemale\n18\n18 to 35\nPC 17757\n227.5250\nC62 C64\nCherbourg\n\n\n2\n0\nLamb, Mr. John Joseph\nmale\nNA\nUnknown\n240261\n10.7083\nNA\nQueenstown\n\n\n2\n1\nLaroche, Miss. Louise\nfemale\n1\n0 to 17\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n2\n1\nLaroche, Miss. Simonne Marie Anne Andree\nfemale\n3\n0 to 17\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n2\n0\nLaroche, Mr. Joseph Philippe Lemercier\nmale\n25\n18 to 35\nSC/Paris 2123\n41.5792\nNA\nCherbourg\n\n\n3\n0\nBarry, Miss. Julia\nfemale\n27\n18 to 35\n330844\n7.8792\nNA\nQueenstown\n\n\n3\n0\nBarton, Mr. David John\nmale\n22\n18 to 35\n324669\n8.0500\nNA\nSouthampton\n\n\n3\n0\nBeavan, Mr. William Thomas\nmale\n19\n18 to 35\n323951\n8.0500\nNA\nSouthampton\n\n\n\n\n\n\n\n\n\nIdentifying categorical data\nIn this chapter, we are focusing on categorical data, so the first step is determining which columns in our dataset contain categorical data and what type of categorical data (nominal or ordinal) they are. In the following video, I walk you through this process:\n\n\n\n\nSummarizing categorical data\nThere is not a lot that you can do with a single categorical variable other than reporting the frequency (count) and relative frequency (percentage) of observations for each category. To do this, we are going to use a tool that we have already learned: pivot tables.\n\nFrequency\nA typical tool to analyze categorical data is to count the number of observation that fall into each of the groups.\n\n\n\nFrequency of passengers by port of embarkation\n\n\nembarked\nfreq\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\nNA\n2\n\n\n\n\n\n\n\n\n\n\nRelative frequency\nThe relative frequency is simply the frequency represented as a percentage rather than count. It is obtained by first computing the frequency and then calculating the relative frequency by dividing each count by the total.\n\n\n\nrelative frequency of passengers by port of embarkation\n\n\nembarked\nfreq\nrel_freq\n\n\n\n\nCherbourg\n270\n0.2062643\n\n\nQueenstown\n123\n0.0939649\n\n\nSouthampton\n914\n0.6982429\n\n\nNA\n2\n0.0015279\n\n\n\n\n\n\n\n\n\n\nRounding the values\nWhen we calculate the relative frequency, we obtain numbers with a lot of decimals. We can remove or add decimals by selecting our data and clicking on the or  buttons, respectively.\n\n\nConverting the relative frequency to percentages\nAnother thing we might want to do is show the relative frequency as a percentage. This can be done by clicking the % button in excel.\n\n\n\n\nrelative frequency of passengers by port of embarkation\n\n\nembarked\nfreq\nrel_freq (%)\n\n\n\n\nCherbourg\n270\n20.6\n\n\nQueenstown\n123\n9.4\n\n\nSouthampton\n914\n69.8\n\n\nNA\n2\n0.2\n\n\n\n\n\n\n\n\n\n\nDemo\nIn this video, I show you how to summarize categorical data using the frequency and the relative frequency.\n\n\n\n\n\nConverting numerical data into categorical data\nJust because a column in a dataset does not contain categorical data, doesn’t mean it can’t be transformed into categorical data. For examples, age groups (e.g. 0-17, 18-35, etc.) are groups (categorical) based on age (numerical). Here’s a video showing we can make that transformation in our Titanic dataset.\n\n\n\n\n\n11.2.2 Analyzing numerical data\nWhile summarizing categorical data is done with frequency and relative frequency tables, numerical data can be summarized using descriptive statistics divided into three groups:\n\nMeasures of central tendency.\nMeasures of dispersion.\nMeasures of skewness.\n\nIn this chapter, we will focus on only the first two: measures of central tendency and measures of dispersion.\n\nMeasures of central tendency\nMeasures of central tendency help you summarize data by looking at the central point within it. While the mode (the value(s) with the highest frequency) is also considered a measure of central tendency, in this chapter we will only consider two: the average (or mean), and the median.\n\n\n\n\n\n\n\n\n\nStatistic\ndescription\nformula\nExcel function\n\n\n\n\nAverage or Mean\nThe sum of values divided by the number of observations\n\\[ \\overline{X}  = \\frac{\\sum{X}}{n} \\]\n=AVERAGE(x:x)\n\n\nMedian\nthe middle value of a set of number once sorted in ascending or descending order\nIf n is odd:\n\\[ M_x = x_\\frac{n + 1}{2} \\]\nIf n is even:\n\\[ M_x = \\frac{x_{(n/2)} + x_{(n/2)+1}}{2} \\]\n=MEDIAN(x:x)\n=PERCENTILE(x:x,0.50)\n=QUARTILE(x:x,2)\n\n\n\nTable 8.1. Measures of central tendency\n\nDemo\nIn the following video, I explain the how to manually calculate the average and median in a set of numerical data.\n\n\n\n\n\nMeasures of dispersion\n\n\n\n\n\n\n\n\n\nStatistic\nDefinition\nFormula\nExcel formula\n\n\n\n\nVariance (Var)\nExpected squared deviation from the mean. Measures how far numbers spread around the average\n\\[ Var = \\frac{\\sum{(x_i-\\overline{x})^2}}{N} \\]\n=VAR(x:x)\n\n\nStandard deviation (SD)\nSquare root of the Variance.\n\\[ SD = \\sqrt{\\frac{\\sum{(x_i-\\overline{x})^2}}{N}} \\]\n=STDEV(x)\n\n\nRange\nDifference between the minimum and maximum values in the data.\nN/A\nMinimum value: =MIN(x:x)\nMaximum value: =MAX(x:x)\nRange: =MAX(x:x)-MIN(x:x)\n\n\nInterquartile range (IQR)\nThe difference between the first quartile (Q1) and the third quartile (Q3). It measures the spread of the middle 50% of the data.\nN/A\nQ1: =QUARTILE(x:x,1)\nQ3: =QUARTILE(x:x,3)\nIQR: =Q3-Q1\n\n\n\nTable 8.2. Measures of dispersion\n\nDemo\nIn the following video, I explain the how to manually calculate the different measures of dispersion in a set of numerical data.\n\n\n\n\n\nCreating a descriptive statistics summary\nUsing the measures of central tendency and dispersion listed above, we can construct a descriptive statistics summary for any number of numerical variables in a dataset. Note that these summaries don’t typically display the interquartile range (IQR) and the range, but instead provide the actual values required to calculate them (i.e., Min, Q1, Q3, Max).\n\n\n\nTable 8.3. Descriptive statistics summary of age and fare in the Titanic dataset.\n\n\nvariable\nN\nMean\nSD\nVAR\nQ1\nMedian\nQ3\nMin\nMax\n\n\n\n\nage\n1046\n29.898\n14.415\n207.791\n21.000\n28.000\n39.000\n0\n80.000\n\n\nfare\n1308\n33.295\n51.759\n2678.960\n7.896\n14.454\n31.275\n0\n512.329\n\n\n\n\n\n\n\n\nDemo\nIn the following video, I show you how to create descriptive statistics summary in Excel.\n\n\n\n\n\n\n11.2.3 Measuring the relationship between two variables\n\nTwo categorical variables\nWhen we want to combine multiple categorical variable to analyze them jointly, we can use a contingency table (aka cross-tabulation or crosstab), which displays the frequency (or relative frequency) distribution two variables. Here’s an example from the Titanic dataset, where we combined the Embark and the Sex columns\n\n\n\nTable 9.1. Frequency of male and female Titanic passengers by port of embarkation\n\n\n\nfemale\nmale\nSum\n\n\n\n\nCherbourg\n113\n157\n270\n\n\nQueenstown\n60\n63\n123\n\n\nSouthampton\n291\n623\n914\n\n\nSum\n464\n843\n1307\n\n\n\n\n\n\n\nIn this table:\n\nThe column represent the categories of one variable (sex: male and female).\nThe rows represent the categories of another variable (Embark: Cherbourg, Queenstown, Southampton).\nThe cells show the frequency counts of the combinations of these categories.\n\n\nDemo\n\n\n\n\n\nA categorical and a numerical variable\nAs we saw in Chapter 8, numerical data can be summarized using a descriptive statistics summary that includes measures of central tendency and measures of dispersion. To summarize the relationship between a categorical variable and a numerical, we simply have to calculate these measures for each group of the categorical variable. For example, the table belows shows the descriptive statistics summary for the fare variable for each passenger class.\n\n\n\nTable 9.2. Descriptive statistics summary of fare for each passenger class\n\n\npclass\nN\nMean\nSD\nVAR\nQ1\nMedian\nQ3\nMin\nMax\n\n\n\n\n1\n323\n87.509\n80.447\n6471.748\n30.696\n60.000\n107.662\n0\n512.329\n\n\n2\n277\n21.179\n13.607\n185.154\n13.000\n15.046\n26.000\n0\n73.500\n\n\n3\n708\n13.303\n11.494\n132.120\n7.750\n8.050\n15.246\n0\n69.550\n\n\n\n\n\n\n\n\nDemo\n\n\n\n\n\nTwo numerical variables\nThe relationship between two numerical variable can be captured by a single number called the correlation coefficient. It is calculated using the CORREL(x:x,y:y) function in Excel, where x:x is a column of numerical data, and y:y is another column of numerical data.\nSuppose you have the following data:\n\n\n\n\n\nA\nB\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n3\n6\n\n\n4\n8\n\n\n5\n10\n\n\n\n\n\n\n\nTo find the correlation coefficient between columns A and B, you would type =CORREL(A1:A5, B1:B5) in a new cell. The result will be 1, which indicates a perfect positive correlation. 0 would mean that there is no correlation at all (no relationship between the two variables), and -1 would indicate a perfect negative correlation. The closer the value is to 1 or -1, the stronger the linear relationship between the two variables.\n\nDemo",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#regression",
    "href": "ch11.html#regression",
    "title": "11  Quantitative data analysis",
    "section": "11.4 Regression",
    "text": "11.4 Regression\n\n11.4.1 Logistic regression\nRegression is a method used to determine the relationship between a dependent variable (the variable we want to predict) and one or more independent variables (the predictors available to make the prediction). There are a wide variety of regression methods, but in this course we will learn two: the logistic regression, which is used to predict a categorical dependent variable, and the linear regression, which is used to predict a continuous dependent variable.\nIn this chapter, we focus on the binomial logistic regression (we will refer to it as logistic regression or simply regression in the rest of the chapter), which means that our dependent variable is dichotomous (e.g., yes or no, pass vs fail). Ordinal logistic regression (for ordinal dependent variables) and multinominal logistic regression (for variables with more than 2 categories) are beyond the scope of the course.\n\nLogistic function\nLogistic regression is called this way because it fits a logistic function (an s-shaped curve) to the data to model the relationship between the predictors and a categorical outcome. More specifically, it models the probability of an outcome (the dependent categorical variable) based on the value of the independent variable. Here’s an example:\n\nThe model gives us, for each value of the independent variable (Copper content, in this example), the probability (odds) that the painting is an original. The point where the logistic curve reaches .5 (50%) on the y-axis is where the cut-off happens: the model predicts that any painting with a copper content above that point is an original.\n\n\nOdds\nWe can convert probabilities into odds by dividing the probability p of the one outcome by the probability of the other outcome, so because there are only two outcomes, then odds = p / 1-p. For example, say we have a bag of 10 balls, 2 red and 8 black. If we draw a ball at random, we have a 8/10 = 80% chance of drawing a black ball. The odds of drawing a black ball are thus 0.8/(1-0.8) = 0.8/0.2 = 4. There is a 4 to 1 chance that we’ll draw a black ball over a red one.\nHowever, the output of the logistic regression model is the natural logarithm of the odds: log odds = ln(p/1-p), so it is not so easily interpreted.\n\n\nLogistic regression example\nHere we will use the Titanic passenger dataset.\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nage group\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n1\nAllen, Miss. Elisabeth Walton\nfemale\n29\n18 to 35\n24160\n211.3375\nB5\nSouthampton\n\n\n1\n1\nAllison, Master. Hudson Trevor\nmale\n1\n0 to 17\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Miss. Helen Loraine\nfemale\n2\n0 to 17\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30\n18 to 35\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25\n18 to 35\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n1\nAnderson, Mr. Harry\nmale\n48\n36 to 59\n19952\n26.5500\nE12\nSouthampton\n\n\n\n\n\n\n\n\nChoose a set of predictors (independent variables)\nLooking at our dataset, we can identify some variables that we think might affect the probability that a passenger survived. In our model, we will choose Sex, Age, Pclass, and Fare.. We can now remove the variables that we don’t need in our model by selecting the ones we want to keep.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n\n\n1\nmale\n1\n151.5500\n1\n\n\n0\nfemale\n2\n151.5500\n1\n\n\n0\nmale\n30\n151.5500\n1\n\n\n0\nfemale\n25\n151.5500\n1\n\n\n1\nmale\n48\n26.5500\n1\n\n\n\n\n\n\n\n\n\nDealing with missing data\nWe can count the number of empty cells for each variable to see if some data is missing. We do this for each variable in the set.\n\n\n\n\n\nvariable\nmissing_values\n\n\n\n\nsurvived\n0\n\n\nsex\n0\n\n\nage\n263\n\n\nfare\n1\n\n\npclass\n0\n\n\n\n\n\n\n\nOnce we have identified that some columns contain missing data, we have two choices. We do nothing and these cases will be left out of the regression model, or we fill the empty cells in some way (this is called imputation). We have many missing values (177 out of 891 observations is quite large) and leaving out these observations could negatively affect the performance of our regression model. Therefore, we will assign the average age for all 177 missing age values, which is a typical imputation mechanism to replace missing values with an estimate based on the available data.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n\n\n1\nmale\n1\n151.5500\n1\n\n\n0\nfemale\n2\n151.5500\n1\n\n\n0\nmale\n30\n151.5500\n1\n\n\n0\nfemale\n25\n151.5500\n1\n\n\n1\nmale\n48\n26.5500\n1\n\n\n\n\n\n\n\n\n\nVisualizing the relationships\nTo explore the relationship between variables. we can visualize the distribution of independent variable values for each value of the dependent variable. We can use box plots for continuous independent variables and bar charts for the categorical variables.\n\n\n\n\n\nsurvived\nVariable\nValue\n\n\n\n\n1\nsex\nfemale\n\n\n1\nsex\nmale\n\n\n0\nsex\nfemale\n\n\n0\nsex\nmale\n\n\n0\nsex\nfemale\n\n\n1\nsex\nmale\n\n\n\n\n\n\n\nLet’s create box plots for all our independent variables and outcome.\n\n\n\n\n\n\n\n\n\nAnd let’s make bar charts for our categorical independent variables.\n\n\n\n\n\n\n\n\n\n\n\nCreating the model\nThe following code generates our logistic regression model using the glm() function (glm stands for general linear model). The syntax is gml(predicted variable ~ predictor1 + predictor2 + preductor3..., data, family) where data is our dataset and the family is the type of regression model we want to create. In our case, the family is binomial.\n\n\nModel summary\nNow that we have created our model, we can look at the coefficients (estimates) which tell us about the relationship between our predictors and the predicted variable. The Pr(&gt;|z|) column represents the p-value, which determines whether the effect observed is statistically significant. It is common to use 0.05 as the threshold for statistical significance, so all the effects in our model are statistically significant (p &lt; 0.05) except for the fare (p &gt; 0.05).\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n4.2805940\n0.4116103\n10.3996285\n0.0000000\n\n\nsexmale\n-2.4893690\n0.1497232\n-16.6264796\n0.0000000\n\n\nage\n-0.0319317\n0.0060478\n-5.2798944\n0.0000001\n\n\nfare\n0.0007146\n0.0016281\n0.4389321\n0.6607108\n\n\npclass\n-1.0410960\n0.1097710\n-9.4842577\n0.0000000\n\n\n\n\n\n\n\n\n\nConverting log odds to odds ratio\nAs we mentioned above, the coefficients produced by the model are log odds, which are difficult to interpret. We can convert them to odds ratio, which are easier to interpret. We can now see that according to our model, female passengers were 12 times more likely to survive than male passengers.\n\n\n\n\n\n(Intercept)\nsexmale\nage\nfare\npclass\n\n\n\n\n72.28337\n0.0829623\n0.9685727\n1.000715\n0.3530675\n\n\n\n\n\n\n\n\n\nAdding confidence intervals\nThe confidence intervals are an estimation of the precision odds ratio. In the example below, We use a 95% confidence interval which means that we are 95% of our estimated coefficients for a predictor are between the 2.5th percentile and the 97.5th percentile (the two values reported in the tables). If we were using a sample to make claims about a population, which does not really apply here due to the unique case of the titanic, we could then think of the confidence interval as indicating a 95% probability that the true coefficient for the entire population is situated in between the two values.\n\n\n\n\n\n\nodds_ratio\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n72.2833656\n32.6163916\n164.0005638\n\n\nsexmale\n0.0829623\n0.0615772\n0.1107899\n\n\nage\n0.9685727\n0.9570331\n0.9800127\n\n\nfare\n1.0007149\n0.9975736\n1.0040235\n\n\npclass\n0.3530675\n0.2840577\n0.4369938\n\n\n\n\n\n\n\n\n\nModel predictions\nFirst we add to our data the probability that the passenger survived as calculated by the model.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\nprobability\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n0.9216158\n\n\n1\nmale\n1\n151.5500\n1\n0.6956141\n\n\n0\nfemale\n2\n151.5500\n1\n0.9638736\n\n\n0\nmale\n30\n151.5500\n1\n0.4751405\n\n\n0\nfemale\n25\n151.5500\n1\n0.9275404\n\n\n1\nmale\n48\n26.5500\n1\n0.3178611\n\n\n\n\n\n\n\nThen we obtain the prediction by creating a new variable called prediction and setting the value to 1 if the calculated probability of survival is greater than 50% and 0 otherwise.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\nprobability\nprediction\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n0.9216158\n1\n\n\n1\nmale\n1\n151.5500\n1\n0.6956141\n1\n\n\n0\nfemale\n2\n151.5500\n1\n0.9638736\n1\n\n\n0\nmale\n30\n151.5500\n1\n0.4751405\n0\n\n\n0\nfemale\n25\n151.5500\n1\n0.9275404\n1\n\n\n1\nmale\n48\n26.5500\n1\n0.3178611\n0\n\n\n\n\n\n\n\nThen we can print a table comparing the model’s prediction to the real data.\n\n\n        predicted\nsurvived   0   1\n       0 680 128\n       1 161 339\n\n\nFinally, we can calculate how well our model fits the data (how good was it at predicting the independent variable) by testing, for each passenger in the dataset, whether the prediction matched the actual outcome for that passenger. Because that test gives TRUE (1) or FALSE (0) for every row of the dataset, calculating the mean of the test results gives us the percentage of correct predictions, which in our case is about 78%.\n\n\n\n\n11.4.2 Linear regression\nLinear regression uses a straight line to model the relationship between categorical or numerical predictors (independent variables) and a numerical predicted value (dependent variable). for a single predictor variable, the formula for the prediction is:\n\\[ y = intercept + slope × x \\]\nIn statistical terms, the same formula is written like this:\n\\[ y = β_0 + βx \\]\nAnd if we have multiple independent variables, the formula becomes:\n\\[ y = β_0 + β_1x_1 + β_2x_2 .... β_nx_n \\]\nTo determine how well the model fits the data (how well does x predict y. the linear model uses the square of the residuals (r2). The residuals are the difference between the predicted values and the real data, measured by the vertical distance between the line and the data points. Here’s a figure from (rhys2020?) to help you visualize this.\n\n\n\n\n\nWe can see in this figure that the intercept is where the line crosses the y-axis. The slope is calculated by dividing the difference in the predicted value of y by the difference in the value of x.\nWhen working with categorical predictors, the intercept is the mean value of the base category, and the slope is the difference between the means of each category. Here’s an example taken again from (rhys2020?).\n\n\n\n\n\n\nBuilding a linear regression model\nLet’s use on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013 to try to build a model that will predicted delayed arrival. For this we will use the flights dataset included in the nycflights13 package. We will consider the following variables in our model:\n\norigin: airport of departure (JFK, LGA, EWR)\ncarrier (we will only compare United Airlines - UA, and American Airlines - AA)\ndistance: flight distance in miles.\ndep_delay: delay of departure in minutes\narr_delay: delay of arrival in minutes (this is our independent variable)\n\n\n\n\n\n\norigin\ncarrier\ndistance\ndep_delay\narr_delay\n\n\n\n\nEWR\nUA\n1400\n2\n11\n\n\nLGA\nUA\n1416\n4\n20\n\n\nJFK\nAA\n1089\n2\n33\n\n\nEWR\nUA\n719\n-4\n12\n\n\nLGA\nAA\n733\n-2\n8\n\n\nJFK\nUA\n2475\n-2\n7\n\n\n\n\n\n\n\n\n\nVisualizing the relationship between the variables\nWe can explore the relationship between our independent variables and our independent variable. How we will approach this depends on the type of independent variables we have.\n\nContinuous independent variables\nFor continuous independent variables, we do scatter plots with a fitted regression line. We can see in the plot below that there appears to be a linear relationship between the delay at departure and the delay at arrival (which is of course not so surprising). Here we used the stat_poly_line() and the stat_poly_eq() functions from the ggpmisc package to display the slope and the coefficient of determination (R2) of the regression line.\n\n\n\n\n\n\nThe coefficient of determination (R2)\n\n\n\nThe coefficient of determination should not be mistaken for the square of residuals, even thought they have the same notation (R2). The coefficient of determination tells us how well the regression line fits the data. It’s value ranges from 0 to 1. An R2 of 0 means that the linear regression model doesn’t predict your dependent variable any better than just using the average, and a value of 1 indicates that the model perfectly predicts the exact value of the dependent variable.\nAnother way to interpret the coefficient of determination is to consider it as a measure of the variation in the dependent variable is explained (or determined) by the model. For instance, a R2 of 0.80 indicates that 80% of the variation in the dependent variable is explained by the model.\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can see that the the linear regression model using considering only the departure delay explains 79% of the variation in the arrival delay. That makes dep_delay a good predictor in our model. On the other hand, the flight distance explains less then 1% of the arrival delays, as we can see in the next graph. This makes distance a very weak predictor of arrival delays.\n\nDistance\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical independent variables\nTo test the linear relationship between an categorical independent variable and the dependent variable, we use the same approach: a scatterplot with geom_smooth(). However, geom_smooth does not work with factors and requires numerical data. Therefore, we need to convert our factors into numerical variables to visualize the trend line with geom_smooth().\nAlso, when dealing with factors variables with more than two categories (factors with more than 2 levels), we need to choose a base level to which we will compare each of the other levels. In other words, all our graphs should display only two categories.\n\nCarrier\nThe plot below shows that there is hardly any relationship between the carrier and the arrival delay, with the variable explaining less than 1% of the variation in arrival delays.\n\n\n\n\n\n\n\n\n\n\n\n11.4.2.0.0.1 Origin\nSince origin has three levels (EWR, LGA and JFK), we want to plot choose a base level and compare each other level to this one. So let’s choose level 1 (EWR) as our base and compare it with level 2 (LGA).\n\n\n\n\n\n\n\n\n\nAnd then we compare airport 1 (EWR) with aiport 3 (JFK).\n\n\n\n\n\n\n\n\n\nAgain, we can see that the airport from which the flight takes off explains less than 1% of the arrival delays and is therefore a poor predictor.\n\n\n\n\nBuilding the multiple linear regression model\nThe process to build the model is the same as the one we used for the logistic regression in the previous chapter. In fact, the process is simpler here because we do not need to convert the coefficient into odds ratios to make them easier to interpret. Let’s use the lm() function to build the model that predicts delay at arrival based on the distance of the flight, the carrier and the origin (we’ll leave the delay of departure out of the model for now).\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n4.6299672\n0.3577348\n12.942455\n0.0000000\n\n\ndistance\n-0.0007208\n0.0002005\n-3.595645\n0.0003238\n\n\ncarrierAA\n-3.6699113\n0.3907133\n-9.392849\n0.0000000\n\n\noriginLGA\n-0.7238011\n0.4052653\n-1.785993\n0.0741037\n\n\noriginJFK\n1.6725192\n0.4638028\n3.606100\n0.0003110\n\n\n\n\n\n\n\nThe estimate coefficient represents the slope of the linear trend line for each predictor, so we can plug these values into our linear equation.\n\\[ ArrDelay = 4.63 - 0.00distance - 3.67AA - 0.72LGA + 1.67JFK \\]\nWe can see that most coefficients are statistically significant, which appears to indicate that they are good predictors, but let’s hold on for a minute before drawing too hasty conclusions. Look at the Adjusted R-squared (r2). It has a value of 0.001663, which is extremely small and indicate that the model explains less than 1% of the variance in delays. In other words, our model does not at all allow us to make predictions about delays. How can almost all predictors in a model be statistically significant and still be very bad predictors?\n\n\n\n\n\n\nBeware of too large sample\n\n\n\nStatistically significant predictors in a model with low predictive value mostly occur when our data set or sample is too large. What’s a good sample size? a good rule of thumb is 10% of the total observations, with at least ten observations per variable in the model but no more than 1000 observations in total. Let’s do this again with a sample of 500 observations. As shown in the code below, we can use the sample_n() function to create a sample of a specific size that will be used in the lm() function.\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n5.6610012\n4.6068846\n1.2288133\n0.2197257\n\n\ndistance\n-0.0026363\n0.0026491\n-0.9951732\n0.3201382\n\n\ncarrierAA\n-7.3550770\n4.8214132\n-1.5255023\n0.1277725\n\n\noriginLGA\n0.1408608\n5.1793365\n0.0271967\n0.9783138\n\n\noriginJFK\n10.5374689\n5.7291977\n1.8392573\n0.0664761\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\n\n\n\n\n0.0104064\n0.0024097\n\n\n\n\n\n\n\nWe see that the model still does a terrible job at predicting arrival delays, and that none of the predictors are statistically significant (at the p &lt; 0.05 level).\n\nAdding departure delay to the model\nFinally, let’s add the departure delay to the model. We’ve seen in the figure above, in which we plotted the arrival delay against the departure delay, that our data points seemed to follow our trend line, so we can expect that adding this predictor will improve our model.\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-8.8252279\n2.0892546\n-4.2241036\n0.0000286\n\n\ndistance\n-0.0007044\n0.0011693\n-0.6024387\n0.5471585\n\n\ncarrierAA\n2.2377029\n2.6296906\n0.8509377\n0.3952162\n\n\noriginLGA\n-2.6282038\n2.7182565\n-0.9668712\n0.3340815\n\n\noriginJFK\n2.6541591\n2.9647408\n0.8952415\n0.3710940\n\n\ndep_delay\n0.9767458\n0.0244661\n39.9223382\n0.0000000\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\n\n\n\n\n0.7681519\n0.7658053\n\n\n\n\n\n\n\nWe can see that when we consider the delay in the departure, we can more accurately predict the delay at arrival, with around 80% of the variance explained by our model!",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#predicting-a-variable-based-on-multiple-other-variables",
    "href": "ch11.html#predicting-a-variable-based-on-multiple-other-variables",
    "title": "11  Quantitative data analysis",
    "section": "11.4 Predicting a variable based on multiple other variables",
    "text": "11.4 Predicting a variable based on multiple other variables\n\n11.4.1 Logistic regression\nRegression is a method used to determine the relationship between a dependent variable (the variable we want to predict) and one or more independent variables (the predictors available to make the prediction). There are a wide variety of regression methods, but in this course we will learn two: the logistic regression, which is used to predict a categorical dependent variable, and the linear regression, which is used to predict a continuous dependent variable.\nIn this chapter, we focus on the binomial logistic regression (we will refer to it as logistic regression or simply regression in the rest of the chapter), which means that our dependent variable is dichotomous (e.g., yes or no, pass vs fail). Ordinal logistic regression (for ordinal dependent variables) and multinominal logistic regression (for variables with more than 2 categories) are beyond the scope of the course.\n\nLogistic function\nLogistic regression is called this way because it fits a logistic function (an s-shaped curve) to the data to model the relationship between the predictors and a categorical outcome. More specifically, it models the probability of an outcome (the dependent categorical variable) based on the value of the independent variable. Here’s an example:\n\nThe model gives us, for each value of the independent variable (Copper content, in this example), the probability (odds) that the painting is an original. The point where the logistic curve reaches .5 (50%) on the y-axis is where the cut-off happens: the model predicts that any painting with a copper content above that point is an original.\n\n\nOdds\nWe can convert probabilities into odds by dividing the probability p of the one outcome by the probability of the other outcome, so because there are only two outcomes, then odds = p / 1-p. For example, say we have a bag of 10 balls, 2 red and 8 black. If we draw a ball at random, we have a 8/10 = 80% chance of drawing a black ball. The odds of drawing a black ball are thus 0.8/(1-0.8) = 0.8/0.2 = 4. There is a 4 to 1 chance that we’ll draw a black ball over a red one.\nHowever, the output of the logistic regression model is the natural logarithm of the odds: log odds = ln(p/1-p), so it is not so easily interpreted.\n\n\nLogistic regression example\nHere we will use the Titanic passenger dataset.\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nage group\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n1\nAllen, Miss. Elisabeth Walton\nfemale\n29\n18 to 35\n24160\n211.3375\nB5\nSouthampton\n\n\n1\n1\nAllison, Master. Hudson Trevor\nmale\n1\n0 to 17\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Miss. Helen Loraine\nfemale\n2\n0 to 17\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30\n18 to 35\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25\n18 to 35\n113781\n151.5500\nC22 C26\nSouthampton\n\n\n1\n1\nAnderson, Mr. Harry\nmale\n48\n36 to 59\n19952\n26.5500\nE12\nSouthampton\n\n\n\n\n\n\n\n\nChoose a set of predictors (independent variables)\nLooking at our dataset, we can identify some variables that we think might affect the probability that a passenger survived. In our model, we will choose Sex, Age, Pclass, and Fare.. We can now remove the variables that we don’t need in our model by selecting the ones we want to keep.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n\n\n1\nmale\n1\n151.5500\n1\n\n\n0\nfemale\n2\n151.5500\n1\n\n\n0\nmale\n30\n151.5500\n1\n\n\n0\nfemale\n25\n151.5500\n1\n\n\n1\nmale\n48\n26.5500\n1\n\n\n\n\n\n\n\n\n\nDealing with missing data\nWe can count the number of empty cells for each variable to see if some data is missing. We do this for each variable in the set.\n\n\n\n\n\nvariable\nmissing_values\n\n\n\n\nsurvived\n0\n\n\nsex\n0\n\n\nage\n263\n\n\nfare\n1\n\n\npclass\n0\n\n\n\n\n\n\n\nOnce we have identified that some columns contain missing data, we have two choices. We do nothing and these cases will be left out of the regression model, or we fill the empty cells in some way (this is called imputation). We have many missing values (177 out of 891 observations is quite large) and leaving out these observations could negatively affect the performance of our regression model. Therefore, we will assign the average age for all 177 missing age values, which is a typical imputation mechanism to replace missing values with an estimate based on the available data.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n\n\n1\nmale\n1\n151.5500\n1\n\n\n0\nfemale\n2\n151.5500\n1\n\n\n0\nmale\n30\n151.5500\n1\n\n\n0\nfemale\n25\n151.5500\n1\n\n\n1\nmale\n48\n26.5500\n1\n\n\n\n\n\n\n\n\n\nVisualizing the relationships\nTo explore the relationship between variables. we can visualize the distribution of independent variable values for each value of the dependent variable. We can use box plots for continuous independent variables and bar charts for the categorical variables.\n\n\n\n\n\nsurvived\nVariable\nValue\n\n\n\n\n1\nsex\nfemale\n\n\n1\nsex\nmale\n\n\n0\nsex\nfemale\n\n\n0\nsex\nmale\n\n\n0\nsex\nfemale\n\n\n1\nsex\nmale\n\n\n\n\n\n\n\nLet’s create box plots for all our independent variables and outcome.\n\n\n\n\n\n\n\n\n\nAnd let’s make bar charts for our categorical independent variables.\n\n\n\n\n\n\n\n\n\n\n\nCreating the model\nThe following code generates our logistic regression model using the glm() function (glm stands for general linear model). The syntax is gml(predicted variable ~ predictor1 + predictor2 + preductor3..., data, family) where data is our dataset and the family is the type of regression model we want to create. In our case, the family is binomial.\n\n\nModel summary\nNow that we have created our model, we can look at the coefficients (estimates) which tell us about the relationship between our predictors and the predicted variable. The Pr(&gt;|z|) column represents the p-value, which determines whether the effect observed is statistically significant. It is common to use 0.05 as the threshold for statistical significance, so all the effects in our model are statistically significant (p &lt; 0.05) except for the fare (p &gt; 0.05).\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n4.2805940\n0.4116103\n10.3996285\n0.0000000\n\n\nsexmale\n-2.4893690\n0.1497232\n-16.6264796\n0.0000000\n\n\nage\n-0.0319317\n0.0060478\n-5.2798944\n0.0000001\n\n\nfare\n0.0007146\n0.0016281\n0.4389321\n0.6607108\n\n\npclass\n-1.0410960\n0.1097710\n-9.4842577\n0.0000000\n\n\n\n\n\n\n\n\n\nConverting log odds to odds ratio\nAs we mentioned above, the coefficients produced by the model are log odds, which are difficult to interpret. We can convert them to odds ratio, which are easier to interpret. We can now see that according to our model, female passengers were 12 times more likely to survive than male passengers.\n\n\n\n\n\n(Intercept)\nsexmale\nage\nfare\npclass\n\n\n\n\n72.28337\n0.0829623\n0.9685727\n1.000715\n0.3530675\n\n\n\n\n\n\n\n\n\nAdding confidence intervals\nThe confidence intervals are an estimation of the precision odds ratio. In the example below, We use a 95% confidence interval which means that we are 95% of our estimated coefficients for a predictor are between the 2.5th percentile and the 97.5th percentile (the two values reported in the tables). If we were using a sample to make claims about a population, which does not really apply here due to the unique case of the titanic, we could then think of the confidence interval as indicating a 95% probability that the true coefficient for the entire population is situated in between the two values.\n\n\n\n\n\n\nodds_ratio\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n72.2833656\n32.6163916\n164.0005638\n\n\nsexmale\n0.0829623\n0.0615772\n0.1107899\n\n\nage\n0.9685727\n0.9570331\n0.9800127\n\n\nfare\n1.0007149\n0.9975736\n1.0040235\n\n\npclass\n0.3530675\n0.2840577\n0.4369938\n\n\n\n\n\n\n\n\n\nModel predictions\nFirst we add to our data the probability that the passenger survived as calculated by the model.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\nprobability\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n0.9216158\n\n\n1\nmale\n1\n151.5500\n1\n0.6956141\n\n\n0\nfemale\n2\n151.5500\n1\n0.9638736\n\n\n0\nmale\n30\n151.5500\n1\n0.4751405\n\n\n0\nfemale\n25\n151.5500\n1\n0.9275404\n\n\n1\nmale\n48\n26.5500\n1\n0.3178611\n\n\n\n\n\n\n\nThen we obtain the prediction by creating a new variable called prediction and setting the value to 1 if the calculated probability of survival is greater than 50% and 0 otherwise.\n\n\n\n\n\nsurvived\nsex\nage\nfare\npclass\nprobability\nprediction\n\n\n\n\n1\nfemale\n29\n211.3375\n1\n0.9216158\n1\n\n\n1\nmale\n1\n151.5500\n1\n0.6956141\n1\n\n\n0\nfemale\n2\n151.5500\n1\n0.9638736\n1\n\n\n0\nmale\n30\n151.5500\n1\n0.4751405\n0\n\n\n0\nfemale\n25\n151.5500\n1\n0.9275404\n1\n\n\n1\nmale\n48\n26.5500\n1\n0.3178611\n0\n\n\n\n\n\n\n\nThen we can print a table comparing the model’s prediction to the real data.\n\n\n        predicted\nsurvived   0   1\n       0 680 128\n       1 161 339\n\n\nFinally, we can calculate how well our model fits the data (how good was it at predicting the independent variable) by testing, for each passenger in the dataset, whether the prediction matched the actual outcome for that passenger. Because that test gives TRUE (1) or FALSE (0) for every row of the dataset, calculating the mean of the test results gives us the percentage of correct predictions, which in our case is about 78%.\n\n\n\n\n11.4.2 Linear regression\nLinear regression uses a straight line to model the relationship between categorical or numerical predictors (independent variables) and a numerical predicted value (dependent variable). for a single predictor variable, the formula for the prediction is:\n\\[ y = intercept + slope × x \\]\nIn statistical terms, the same formula is written like this:\n\\[ y = β_0 + βx \\]\nAnd if we have multiple independent variables, the formula becomes:\n\\[ y = β_0 + β_1x_1 + β_2x_2 .... β_nx_n \\]\nTo determine how well the model fits the data (how well does x predict y. the linear model uses the square of the residuals (r2). The residuals are the difference between the predicted values and the real data, measured by the vertical distance between the line and the data points. Here’s a figure from Rhys (2020) to help you visualize this.\n\n\n\n\n\nWe can see in this figure that the intercept is where the line crosses the y-axis. The slope is calculated by dividing the difference in the predicted value of y by the difference in the value of x.\nWhen working with categorical predictors, the intercept is the mean value of the base category, and the slope is the difference between the means of each category. Here’s an example taken again from Rhys (2020).\n\n\n\n\n\n\nBuilding a linear regression model\nLet’s use on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013 to try to build a model that will predicted delayed arrival. For this we will use the flights dataset included in the nycflights13 package. We will consider the following variables in our model:\n\norigin: airport of departure (JFK, LGA, EWR)\ncarrier (we will only compare United Airlines - UA, and American Airlines - AA)\ndistance: flight distance in miles.\ndep_delay: delay of departure in minutes\narr_delay: delay of arrival in minutes (this is our independent variable)\n\n\n\n\n\n\norigin\ncarrier\ndistance\ndep_delay\narr_delay\n\n\n\n\nEWR\nUA\n1400\n2\n11\n\n\nLGA\nUA\n1416\n4\n20\n\n\nJFK\nAA\n1089\n2\n33\n\n\nEWR\nUA\n719\n-4\n12\n\n\nLGA\nAA\n733\n-2\n8\n\n\nJFK\nUA\n2475\n-2\n7\n\n\n\n\n\n\n\n\n\nVisualizing the relationship between the variables\nWe can explore the relationship between our independent variables and our independent variable. How we will approach this depends on the type of independent variables we have.\n\nContinuous independent variables\nFor continuous independent variables, we do scatter plots with a fitted regression line. We can see in the plot below that there appears to be a linear relationship between the delay at departure and the delay at arrival (which is of course not so surprising). We can display the slope and the coefficient of determination (R2) of the regression line.\n\n\n\n\n\n\nThe coefficient of determination (R2)\n\n\n\nThe coefficient of determination should not be mistaken for the square of residuals, even thought they have the same notation (R2). The coefficient of determination tells us how well the regression line fits the data. It’s value ranges from 0 to 1. An R2 of 0 means that the linear regression model doesn’t predict your dependent variable any better than just using the average, and a value of 1 indicates that the model perfectly predicts the exact value of the dependent variable.\nAnother way to interpret the coefficient of determination is to consider it as a measure of the variation in the dependent variable is explained (or determined) by the model. For instance, a R2 of 0.80 indicates that 80% of the variation in the dependent variable is explained by the model.\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the the linear regression model using considering only the departure delay explains 79% of the variation in the arrival delay. That makes dep_delay a good predictor in our model. On the other hand, the flight distance explains less then 1% of the arrival delays, as we can see in the next graph. This makes distance a very weak predictor of arrival delays.\n\nDistance\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical independent variables\nTo test the linear relationship between a categorical independent variable and the dependent variable, we can use a similar approach: a scatterplot with a linear regression line. However, to do this we need to transform the categorical variable into a numeric variable (in the graph, you will see the airports represented by a number). Also, When dealing with factors variables with more than two categories (factors with more than 2 levels), we need to choose a base level to which we will compare each of the other levels. In other words, all our graphs should display only two categories.\n\nCarrier\nThe plot below shows that there is hardly any relationship between the carrier and the arrival delay, with the variable explaining less than 1% of the variation in arrival delays.\n\n\n\n\n\n\n\n\n\n\n\nOrigin\nSince origin has three levels (EWR, LGA and JFK), we want to plot choose a base level and compare each other level to this one. So let’s choose level 1 (EWR) as our base and compare it with level 2 (LGA).\n\n\n\n\n\n\n\n\n\nAnd then we compare airport 1 (EWR) with aiport 3 (JFK).\n\n\n\n\n\n\n\n\n\nAgain, we can see that the airport from which the flight takes off explains less than 1% of the arrival delays and is therefore a poor predictor.\n\n\n\n\nBuilding the multiple linear regression model\nThe process to build the model is the same as the one we used for the logistic regression in the previous chapter. In fact, the process is simpler here because we do not need to convert the coefficient into odds ratios to make them easier to interpret. We can build the model that predicts delay at arrival based on the distance of the flight, the carrier and the origin (we’ll leave the delay of departure out of the model for now).\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n4.6299672\n0.3577348\n12.942455\n0.0000000\n\n\ndistance\n-0.0007208\n0.0002005\n-3.595645\n0.0003238\n\n\ncarrierAA\n-3.6699113\n0.3907133\n-9.392849\n0.0000000\n\n\noriginLGA\n-0.7238011\n0.4052653\n-1.785993\n0.0741037\n\n\noriginJFK\n1.6725192\n0.4638028\n3.606100\n0.0003110\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\n\n\n\n\n0.0017078\n0.0016633\n\n\n\n\n\n\n\nThe estimate coefficient represents the slope of the linear trend line for each predictor, so we can plug these values into our linear equation.\n\\[ ArrDelay = 4.63 - 0.00distance - 3.67AA - 0.72LGA + 1.67JFK \\]\nWe can see that most coefficients are statistically significant, which appears to indicate that they are good predictors, but let’s hold on for a minute before drawing too hasty conclusions. Look at the Adjusted R-squared (r2). It has a value of 0.001663, which is extremely small and indicate that the model explains less than 1% of the variance in delays. In other words, our model does not at all allow us to make predictions about delays. How can almost all predictors in a model be statistically significant and still be very bad predictors?\n\n\n\n\n\n\nBeware of too large sample\n\n\n\nStatistically significant predictors in a model with low predictive value mostly occur when our data set or sample is too large. What’s a good sample size? a good rule of thumb is 10% of the total observations, with at least ten observations per variable in the model but no more than 1000 observations in total. Let’s do this again with a sample of 500 observations. We can use the create a sample of a specific size that will be used in the lm() function.\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n1.7864104\n4.825768\n0.3701816\n0.7114056\n\n\ndistance\n0.0010263\n0.002696\n0.3806790\n0.7036047\n\n\ncarrierAA\n-11.7890174\n5.456539\n-2.1605301\n0.0312108\n\n\noriginLGA\n8.7171575\n5.586179\n1.5604865\n0.1192840\n\n\noriginJFK\n12.4601593\n6.401921\n1.9463157\n0.0521823\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\n\n\n\n\n0.0121575\n0.0041749\n\n\n\n\n\n\n\nWe see that the model still does a terrible job at predicting arrival delays, and that none of the predictors are statistically significant (at the p &lt; 0.05 level).\n\nAdding departure delay to the model\nFinally, let’s add the departure delay to the model. We’ve seen in the figure above, in which we plotted the arrival delay against the departure delay, that our data points seemed to follow our trend line, so we can expect that adding this predictor will improve our model.\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-6.3578887\n2.2143259\n-2.871253\n0.0042641\n\n\ndistance\n-0.0016663\n0.0012631\n-1.319249\n0.1876969\n\n\ncarrierAA\n-4.6325615\n2.3377374\n-1.981643\n0.0480731\n\n\noriginLGA\n4.1617315\n2.4576793\n1.693358\n0.0910177\n\n\noriginJFK\n6.6899799\n2.8138690\n2.377502\n0.0178100\n\n\ndep_delay\n1.0215813\n0.0159641\n63.992388\n0.0000000\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\n\n\n\n\n0.8927765\n0.8916913\n\n\n\n\n\n\n\nWe can see that when we consider the delay in the departure, we can more accurately predict the delay at arrival, with around 80% of the variance explained by our model!",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#references",
    "href": "ch11.html#references",
    "title": "11  Quantitative data analysis",
    "section": "11.5 References",
    "text": "11.5 References\n\n\n\n\nRhys, Hefin. 2020. Machine Learning with r, the Tidyverse, and Mlr. Shelter Island, NY: Manning publications.",
    "crumbs": [
      "Analytical phase",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  }
]